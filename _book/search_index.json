[["index.html", "1 AquaMatch data harmonization process", " 1 AquaMatch data harmonization process This bookdown documents the harmonization process for raw data downloaded from the Water Quality Portal (WQP) and used to build the AquaMatch dataset. The data from the WQP includes data obtained from a wide range of acquisition and analysis methods and multiple characteristic names are often of interest to a user studying a particular parameter. In this workflow, we create parameter groups to harmonize by (e.g., “chlorophyll a”) in order to gain a better understanding of how to handle each parameter group. For example chlorophyll data in the WQP includes chlorophyll in any form (a, b, c, corrected for pheophytin, etc.) but only a subset of these will be of interest to us in building our “chlorophyll a” parameter group. Each of these parameters can be retrieved using a variety of methods, some of which are interoperable, and others which are not. This harmonization process allows us to filter out or flag data that may not have enough information for us to consider the value of high quality. This document first describes selections made while downloading WQP data, then describes the “pre-harmonization” process that data for all parameters go through, and then describes how the chlorophyll, DOC, secchi, and TSS parameters are each filtered and harmonized. "],["download-process.html", "2 Download process", " 2 Download process 2.0.1 Catalogue existing data The download process begins by cataloging the existing data that is available in the WQP. To do this the user specifies parameters and their corresponding characteristicNames for retrieving data from the WQP. Below are the current parameters and their characteristicNames as defined in the configuration YAML, 1_inventory/cfg/wqp_codes.yml. Data for these parameters are requested from the WQP within a spatial grid that is mapped in the next section. Code yaml_contents &lt;- yaml::read_yaml(&quot;../1_inventory/cfg/wqp_codes.yml&quot;) names_yaml &lt;- names(yaml_contents) map_df(.x = names_yaml, .f = ~yaml_contents[.x] %&gt;% as_tibble() %&gt;% rename(characteristicName = 1) %&gt;% mutate(parameter = .x) %&gt;% select(parameter, characteristicName)) %&gt;% kable() %&gt;% kable_material() %&gt;% collapse_rows(columns = 1) parameter characteristicName chlorophyll Chlorophyll a chlorophyll Chlorophyll a (probe relative fluorescence) chlorophyll Chlorophyll a, corrected for pheophytin chlorophyll Chlorophyll a (probe) chlorophyll Chlorophyll a, free of pheophytin chlorophyll Chlorophyll a, uncorrected for pheophytin chlorophyll Chlorophyll a - Phytoplankton (suspended) doc Organic carbon doc Total carbon doc Hydrophilic fraction of organic carbon doc Non-purgeable Organic Carbon (NPOC) secchi Depth, Secchi disk depth secchi Depth, Secchi disk depth (choice list) secchi Secchi Reading Condition (choice list) secchi Secchi depth secchi Water transparency, Secchi disc tss Total suspended solids tss Total Particulate Matter tss Total Suspended Particulate Matter 2.0.2 Maps of data spread: Maps are presented below with counts of records across a grid. The grid is how records are grouped in download requests to the Water Quality Portal. Note: The counts here are for raw data that are not filtered or harmonized. Code # Combine counts in each grid_id with the grid polygons grid_counts &lt;- left_join(x = global_grid, y = site_counts %&gt;% count(grid_id), by = c(&quot;id&quot; = &quot;grid_id&quot;)) %&gt;% st_transform(crs = 9311) state_selection &lt;- states(progress_bar = FALSE) %&gt;% st_transform(crs = 9311) ## Retrieving data for the year 2021 Code # Conterminous US map: conterminous_us &lt;- state_selection %&gt;% filter(!(NAME %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;American Samoa&quot;, &quot;Guam&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;))) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = conterminous_us, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(conterminous_us)[,&quot;X&quot;]), max(st_coordinates(conterminous_us)[,&quot;X&quot;])), ylim = c(min(st_coordinates(conterminous_us)[,&quot;Y&quot;]), max(st_coordinates(conterminous_us)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Conterminous US&quot;) + theme_bw() Code # Alaska map: AK &lt;- state_selection %&gt;% filter(NAME == &quot;Alaska&quot;) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = AK, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(AK)[,&quot;X&quot;]), max(st_coordinates(AK)[,&quot;X&quot;])), ylim = c(min(st_coordinates(AK)[,&quot;Y&quot;]), max(st_coordinates(AK)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Alaska&quot;) + theme_bw() Code # Hawaii map: HI &lt;- state_selection %&gt;% filter(NAME == &quot;Hawaii&quot;) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = HI, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(HI)[,&quot;X&quot;]), 0.9 * max(st_coordinates(HI)[,&quot;X&quot;])), ylim = c(1.1 * min(st_coordinates(HI)[,&quot;Y&quot;]), max(st_coordinates(HI)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Hawaii&quot;) + theme_bw() Code # American Samoa map: AS &lt;- state_selection %&gt;% filter(NAME %in% c(&quot;American Samoa&quot;)) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = AS, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(1.025 * min(st_coordinates(AS)[,&quot;X&quot;]), 0.975 *max(st_coordinates(AS)[,&quot;X&quot;])), ylim = c(1.05 * min(st_coordinates(AS)[,&quot;Y&quot;]), max(st_coordinates(AS)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;American Samoa&quot;) + theme_bw() Code # Guam &amp; Commonwealth of the Northern Mariana Islands map GU_CNMI &lt;- state_selection %&gt;% filter(NAME %in% c(&quot;Guam&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;)) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = GU_CNMI, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(1.025 * min(st_coordinates(GU_CNMI)[,&quot;X&quot;]), 0.975 * max(st_coordinates(GU_CNMI)[,&quot;X&quot;])), ylim = c(0.95 * min(st_coordinates(GU_CNMI)[,&quot;Y&quot;]), 1.05 * max(st_coordinates(GU_CNMI)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Guam &amp; Commonwealth of the Northern Mariana Islands&quot;) + theme_bw() Code # Puerto Rico &amp; United States Virgin Islands map: PR_VI &lt;- state_selection %&gt;% filter(NAME %in% c(&quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;)) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = PR_VI, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(0.95 * min(st_coordinates(PR_VI)[,&quot;X&quot;]), 1.05 * max(st_coordinates(PR_VI)[,&quot;X&quot;])), ylim = c(1.075 * min(st_coordinates(PR_VI)[,&quot;Y&quot;]), 0.925 * max(st_coordinates(PR_VI)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Puerto Rico &amp; United States Virgin Islands&quot;) + theme_bw() "],["pre-harmonization-decision-process.html", "3 Pre-harmonization decision process 3.1 Justification", " 3 Pre-harmonization decision process 3.1 Justification Following the completion of the download process described in the previous chapter, the pipeline contains raw data for a variety of potential parameters of interest. Before we begin to harmonize each individual parameter we run through a series of “pre-harmonization” steps, which ensure that each parameter will enter its individual harmonization routine with a dataset that has been formatted identically to each other parameter in the pipeline. 3.1.1 Initial dataset At the start of the pre-harmonization process the WQP dataset contains the parameters chlorophyll, doc, secchi, tss and 14.16 million rows. 3.1.2 Remove duplicates The first filtering step is to remove any duplicated records present in the dataset. Duplicate records occur in the WQP database. The pipeline allows for the user to specify which columns the clean_wqp_data() function should use when checking for duplicated information. By default these columns are: OrganizationIdentifier, MonitoringLocationIdentifier, ActivityStartDate, ActivityStartTime.Time, CharacteristicName, and ResultSampleFractionText. This results in dropping 2.92 million rows for a record count of 11.24 million. 3.1.3 Missing results Next records that have missing data are dropped from the dataset. Several criteria are used when checking for missing data. If any of the below criteria are met the row is flagged as missing: Both the result column and detection limit column had NA data Result, result unit, activity comment, laboratory comment, and result comment columns are all NA Result detection condition column contains not reported The result comment column contains any user-provided text indicating a missing value, currently including: analysis lost, not analyzed, not recorded, not collected, or no measurement taken 409.57 thousand rows are dropped, resulting in a final count of 10.83 million. 3.1.4 Filter status We next filter the ResultStatusIdentifier column to include only the following statuses: \"Accepted\" \"Final\" \"Historical\" \"Validated\" \"Preliminary\" NA These statuses generally indicate a reliable result having been reached, however we also include NA in an effort to be conservative. More specifically, when making decisions for this and other columns we occasionally retain NA values if removing the records would otherwise drop 10% or more of the available data. 44.13 thousand rows are dropped from the dataset leaving it with 10.79 million remaining. 3.1.5 Filter media Next the ActivityMediaSubdivisionName is filtered to only include only the following media types: \"Surface Water\" \"Water\" \"Estuary\" NA 11.59 thousand rows are dropped and 10.78 million remain. 3.1.5.1 Location type The final step in pre-harmonization is filtering out any ResolvedMonitoringLocationTypeName that is not \"Estuary\", \"Lake, Reservoir, Impoundment\", or \"Stream\". After dropping 0 rows the pre-harmonization dataset is complete with 10.78 million. "],["tiering-flagging-and-quality-control-philosophy.html", "4 Tiering, flagging, and quality control philosophy 4.1 NA values 4.2 Heterogenous data", " 4 Tiering, flagging, and quality control philosophy The variety of data providers, parameters, and methods in the WQP inevitably results in a heterogenous dataset that requires quality control before rigorous analytical use. 4.1 NA values Columns related directly to the interoperability of data in the WQP, specifically field and lab methodology and depth-related columns, often contain many NAs. A highly restrictive filtering of the WQP would thus result in very limited data, in part, due to the inconsistent entry of data across data providers. Therefore, we are building this dataset to resolve as many NAs as possible, but to also include NAs within an “inclusive” data tier when necessary. 4.2 Heterogenous data In addition to missing or NA values, WQP data can be heterogenous for reasons such as: Having a variety of methods used across different organizations for the same parameter Having a variety of names or descriptions of the same (e.g., analytical) method across different organizations Containing data for one parameter obtained by multiple methods with varying levels of reliability Having information relevant to assessing data reliability or method choice spread across multiple columns In order to control for some of this variation and provide end-users with something more readily navigable we have incorporated several tiers and flagging systems into the AquaSat v2 dataset: Analytical method tiering: Tiers indicating the interoperability of data based on the analytical method used to acquire measurements Field flags: Flags indicating whether the sample collection method is consistent with the analytical method Depth flags: Flags indicating the completeness of the depth data associated with the measurement 4.2.1 Analytical method tiering The ResultAnalyticalMethod.MethodName column from the WQP is the primary indicator we used for determining which analytical methods tier records fall within. Each parameter’s harmonization chapter in this {bookdown} document contains detail on which methods were sorted into which tiers. The primary concern of our analytical method tiering philosophy was determining the reliability and accuracy of each analytical method for each parameter in the AquaSat v2 dataset. This naturally varies for each parameter of interest and its corresponding characteristicNames in the WQP. We developed the following categories for this column, which is denoted as analytical_tier in the final dataset: Tier 0: Restrictive. Data that are verifiably self-similar across organizations and time-periods and can be considered highly reliable and interoperable Tier 1: Narrowed. Data that we have good reason to believe are self-similar, but for which we can’t verify full interoperability across data providers Tier 2: Inclusive. Data that are assumed to be reliable and are harmonized to our best ability given the information available from the data provider. This tier includes NA or non-resolvable descriptions for the analytical method, which often make up the majority of methods descriptions for any given parameter. 4.2.2 Field flags The field_flag column is used to check if the sample collection method (SampleCollectionMethod.MethodName) is reasonable (flag = 0), suspect (flag = 1), or inconclusive (flag = 2). “Reasonable” flags mean that the sample collection method and the analytical method or tier are in harmony with one another. “Suspect” flags are assigned when the sample collection method description isn’t consistent with what would be expected given the characteristicName and the ResultAnalyticalMethod.MethodName for a given record. “Inconclusive” or unknown flags are assigned to records with “inclusive” analytical tiers because these tiers include values such as NA, where it’s typically impossible to determine the appropriateness of a collection method for the sample record. The relationship between field flags and analytical tiers will vary by parameter and each harmonization chapter contains specific information for these different cases. 4.2.3 Depth flags Depth information in the WQP is surprisingly complex. There are four columns that explicitly contain depth information, all of which are likely to contain values with an extensive variety of measurement units. Below is a list of the columns and their definitions according to the {dataRetrieval} package documentation. ActivityDepthHeightMeasure.MeasureValue: “A measurement of the vertical location (measured from a reference point) at which an activity occurred.” ResultDepthHeightMeasure.MeasureValue: “A measurement of the vertical location (measured from a reference point) at which a result occurred.” Only in STORET ActivityTopDepthHeightMeasure.MeasureValue: “A measurement of the upper vertical location of a vertical location range (measured from a reference point) at which an activity occurred.” ActivityBottomDepthHeightMeasure.MeasureValue: “A measurement of the lower vertical location of a vertical location range (measured from a reference point) at which an activity occurred.” 4.2.3.1 Pre-processing Prior to assigning the depth_flag we complete a few pre-processing steps: Convert the following (character) values to an explicit NA: “NA”, “999”, “-999”, “9999”, “-9999”, “-99”, “99”, “NaN” Convert depths in all four columns to meters from the depth unit listed by the data provider Create a single “discrete” depth column using a combination of the ActivityDepthHeightMeasure.MeasureValue and ResultDepthHeightMeasure.MeasureValue columns. We use ActivityDepth value when ResultDepth value is missing, and ResultDepth when ActivityDepth is missing. If both columns have values but disagree we use an average of the two. This pre-processing results in three “harmonized” columns reporting depth values in meters: harmonized_discrete_depth_value, harmonized_top_depth_value, harmonized_bottom_depth_value. Flags are assigned using the harmonized depth columns that come out of the pre-processing steps above. If the record has no depth listed it is designated depth_flag = 0. A record with only discrete depth is given a depth_flag = 1. A top and/or bottom depth, indicating an integrated sample, is assigned depth_flag = 2, and any combination of discrete + top and/or bottom depths is assigned depth_flag = 3. Included in the 0 flag are situations where there is a disagreement in discrete depth measurements once they are converted into shared units. These values are also recoded as an NA in our harmonized column. The majority of records with a 3 flag have disagreements in values between discrete and integrated columns and are therefore impossible to reconcile with certainty. 4.2.4 General note Generally speaking, we avoid further classification for any observation’s parameter methodology, tier, or flag unless there are at least 1% of observations with the same unresolved text. "],["chlorophyll-harmonization-process.html", "5 Chlorophyll harmonization process", " 5 Chlorophyll harmonization process 5.0.1 Initial dataset After the pre-harmonization process the chlorophyll-only WQP dataset contains 2.63 million rows. This dataset contains the following user-defined characteristicNames: Chlorophyll a, Chlorophyll a (probe relative fluorescence), Chlorophyll a, corrected for pheophytin, Chlorophyll a (probe), Chlorophyll a, free of pheophytin, Chlorophyll a, uncorrected for pheophytin, Chlorophyll a - Phytoplankton (suspended). These names are chosen in order to select for only those measurements that pertain to chlorophyll a. 5.0.2 Filter for water media The first step in chla harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The final row count after this is 2.63 million. 5.0.3 Document and remove fails In this step we filter out records based on indications that they have failed data quality assurance or quality control for some reason given by the data provider (these instances are referred to here as “failures”). After reviewing the contents of the ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, and ResultMeasureValue_originalcolumns, we developed a list of terms that captured the majority of instances where records had failures or unacceptable measurements. We found the phrasing to be consistent across columns and so we search for the same (case agnostic) terms in all four locations. The terms are: “beyond accept”, “cancelled”, “contaminat”, “error”, “fail”, “improper”, “instrument down”, “interference”, “invalid”, “no result”, “no test”, “not accept”, “outside of accept”, “problem”, “QC EXCEEDED”, “questionable”, “suspect”, “unable”, “violation”, “reject”, “no data”. Below are pie charts that break down the number of failure detections by column. Note that the plotting below is automated and so if one or more of the columns listed above are not plotted, this indicates that the column(s) did not return any matches for the failure phrases. Also note that the detection of one of the potential failure phrases is not mutually exclusive from other phrases showing up in the same record within the same column or one of the other three columns. 5.0.3.1 ActivityCommentText fail detects 5.0.3.2 ResultCommentText fail detects 5.0.3.3 ResultLaboratoryCommentText fail detects 69.55 thousand rows are removed after detecting failure-related phrases and 2.56 million rows remain. 5.0.4 Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.56 million. 5.0.5 Clean approximate values Step 4 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.56 million. 5.0.6 Clean values with “greater than” data Step 5 is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.56 million. 5.0.7 Harmonize record units The next step in chla harmonization is converting the units of WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into micrograms/L: ResultMeasure.MeasureUnitCode conversion mg/l 1e+03 mg/L 1e+03 ppm 1e+03 ug/l 1e+00 ug/L 1e+00 mg/m3 1e+00 ppb 1e+00 mg/cm3 1e+06 ug/ml 1e+03 mg/ml 1e+06 ppt 1e+06 117.48 thousand rows are removed. The final row count after this is 2.44 million. 5.0.8 Clean depth data The next harmonization step cleans the four depth-related columns obtained from the WQP. The details behind this step are covered in the Depth flags section of the Tiering, flagging, and quality control philosophy chapter. Through our depth filtering we lose 0 rows and have 2.44 million remaining. 5.0.9 Filter and tier analytical methods We next review the analytical methods used in measuring chlorophyll a, primarily by classifying the text provided with each record in ResultAnalyticalMethod.MethodName. Once these methods are classified we arrange them into hierarchical tiers as described in the Analytical method tiering section of the Tiering, flagging, and quality control philosophy chapter. However, prior to classification we check the ResultAnalyticalMethod.MethodName column for names that indicate non-chlorophyll measurements. We remove these because it’s impossible to interpret contradictory information provided within the same data record. Phrases used to flag and remove unrelated methods from chlorophyll a data are: “sulfate”, “sediment”, “5310”, “counting”, “plasma”, “turbidity”, “coliform”, “carbon”, “2540”, “conductance”, “nitrate”, “nitrite”, “nitrogen”, “alkalin”, “zooplankton”, “phosphorus”, “periphyton”, “peri”, “biomass”, “temperature”, “elemental analyzer”, “2320”. This process drops 66.25 thousand rows leaving 2.38 million remaining. The next step towards creating analytical tiers is to then classify the methods in ResultAnalyticalMethod.MethodName into either: HPLC methods, spectrophotometer and fluorometer methods, or methods for which a pheophytin correction is recorded as part of the methodology. These classifications are not the final tiers, but they inform the tiering in the final step of this process. The criteria for each of the above classifications are: HPLC: Detection of “447”, “chromatography”, or “hplc” in the ResultAnalyticalMethod.MethodName or presence of 70951 or 70953 in the USGSPCode column Spectro/fluoro: Detection of “445”, “fluor”, “Welshmeyer”, “fld”, “10200”, “446”, “trichromatic”, “spectrophoto”, “monochrom”, “monchrom”, or “spec” not as part of a word in ResultAnalyticalMethod.MethodName Pheophytin correction: Detection of “correct”, “445”, “446”, or “in presence” in ResultAnalyticalMethod.MethodName or detection of “corrected for pheophytin” or “free of pheophytin” in CharacteristicName Finally, we group the data into three tiers as described in Tiering, flagging, and quality control philosophy. As a reminder, these tiers are: Tier 0: Restrictive. Data that are verifiably self-similar across organizations and time-periods and can be considered highly reliable and interoperable Tier 1: Narrowed. Data that we have good reason to believe are self-similar, but for which we can’t verify full interoperability across data providers Tier 2: Inclusive. Data that are assumed to be reliable and are harmonized to our best ability given the information available from the data provider. This tier includes NA or non-resolvable descriptions for the analytical method, which often make up the majority of methods descriptions for any given parameter. For chlorophyll a, the following rules are used: Tier 0: Includes records using HPLC methods Tier 1: Spectrophotometer and fluorometer methods that are also pheophytin-corrected OR records where USGSPCode is 32209 Tier 2: All other records by default, including NA methods and in-situ probes At this point we export a file (3_harmonize/out/chla_analytical_tiering_record.csv) that contains a record of how specific method text was tiered and how many row counts corresponded to each method. 5.0.10 Flag based on field methods The final step in our chla harmonization is flagging field sampling methods based primarily on the SampleCollectionMethod.MethodName column. We first classify each record into either in vitro or in situ methods (i.e., in vitro assumes a water sample was collected and take to a lab for analysis; in situ assumes a measurement was taken in place). We used the following strings to mark in vitro samples: “grab”, “bottle”, “vessel”, “bucket”, “jar”, “composite”, “integrate”, “UHL001”, “surface”, “filter”, “filtrat”, “1060B”, “kemmerer”, “collect”, “rosette”, “equal width”, “vertical”, “van dorn”, “bail”, “sample”, “sampling”, “lab” not in the middle of another word, or a “G” on its own as shorthand for “grab”. In situ samples were detected using “in situ”, “probe”, or “ctd”. Lastly we created the field flag based on whether the sampling method used agrees with the analytical method. Flags of 0 indicated that the field sampling method is in agreement with the analytical method, 1 indicates that the field sampling methods are uncharacteristic of the analytical method, and anything with analytical tier of 2 is given a field flag of 2 because of the range of potential analytical methods and corresponding sampling methods possible for the inclusive analytical tier. The following rules are used for chlorophyll a field sampling flags: Flag 0: Restrictive and narrowed analytical tiers with in vitro field methods Flag 1: Restrictive and narrowed analytical tiers with in situ field methods Flag 2: Anything in the inclusive tier No records should be removed by this process and so there are rows dropped leaving million remaining in the final harmonized chla dataset. 5.0.11 Harmonized chlorophyll a At this point the harmonization of the chlorophyll a WQP data is complete and we export the final dataset for use later in the workflow. "],["doc-harmonization-process.html", "6 DOC harmonization process", " 6 DOC harmonization process 6.0.1 Initial dataset After the pre-harmonization process the DOC-only WQP dataset contains 2.2 million rows. 6.0.2 Filter for water media The first step in DOC harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The final row count after this is 2.2 million. 6.0.3 Remove fails and other missing data In this step we filter out records based on indications that they have failed data for some reason. We screen the following columns: ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, ResultMeasureValue, and ResultDetectionConditionText. Examples of text that results in a dropped record includes (but is not limited to): \"fail\", \"suspect\", \"error\", \"beyond accept\", \"interference\", \"questionable\", \"problem\", \"violation\", \"rejected\", \"no data\". Specific target text varies by column. 72.63 thousand rows are removed and 2.13 million rows remain. 6.0.4 Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.13 million. 6.0.5 Clean approximate values Step 4 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.13 million. 6.0.6 Harmonize record units The next step in doc harmonization is working with the units of the WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into milligrams/L: ResultMeasure.MeasureUnitCode conversion mg/L 1.000e+03 mg/l 1.000e+03 ppm 1.000e+03 ug/l 1.000e+00 ug/L 1.000e+00 mg/m3 1.000e+00 ppb 1.000e+00 mg/cm3 1.000e+06 ug/ml 1.000e+03 mg/ml 1.000e+06 ppt 1.000e-06 umol/L 6.008e+01 We also limit values to less than 50 mg/L to ensure realistic data. 17.4 thousand rows are removed. The final row count after this is 2.11 million. 6.0.7 Filter based on analytical method Our next step is to aggregate doc analytical methods into groups and then filter out methods that may have been erroneously added, were unclear, or which don’t meet our needs. Methods that were NA or were grouped as \"Ambiguous\" or \"Carbonaceous Analyzer\" are dropped. This process drops 1053.02 million rows leaving 1.06 million remaining. 6.0.8 Filter based on fraction type The final step in our doc harmonization is filtering based on the ResultSampleFractionText column. We keep records with the following values in this column: \"Dissolved\", \"Filtered, lab\", \"Filterable\", \"Filtered, field\". This process drops 704.11 thousand rows leaving 0.35 million remaining in the final harmonized doc dataset. "],["secchi-harmonization-process.html", "7 Secchi harmonization process", " 7 Secchi harmonization process 7.0.1 Initial dataset After the pre-harmonization process the secchi-only WQP dataset contains 2.62 million rows. 7.0.2 Filter for water media The first step in secchi harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The row count after this is 2.62 million. 7.0.3 Remove fails and other missing data In this step we filter out records based on indications that they have failed data for some reason. We screen the following columns: ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, and ResultMeasureValue. Examples of text that results in a dropped record includes (but is not limited to): \"fail\", \"suspect\", \"error\", \"beyond accept\", \"interference\", \"questionable\", \"problem\", \"violation\", \"rejected\", \"no secchi\". Specific target text varies by column. 19.54 thousand rows are removed and 2.6 million rows remain. 7.0.4 Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The row count after this is 2.6 million. 7.0.5 Clean approximate values Step 4 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The row count after this is 2.6 million. 7.0.6 Clean values with “greater than” data Step 5 is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The row count after this is 2.6 million. 7.0.7 Harmonize record units The next step in secchi harmonization is working with the units of the WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into meters: ResultMeasure.MeasureUnitCode conversion m 1.0000 ft 0.3048 cm 0.0100 in 0.0254 mm 0.0010 mi 1609.3400 We also limit values to less than 15m to ensure realistic data. 3298 rows are removed. The row count after this is 2.6 million. 7.0.8 Filter based on analytical method Our next step is to aggregate secchi analytical methods into groups and then filter out methods that may have been erroneously added, were unclear, or which don’t meet our needs. We accomplish this using an external match table csv file that is joined to the dataset. Methods that are coded \"unlikely\" to be accurate methods are dropped. This process drops 2337 rows leaving 2.6 million remaining. 7.0.9 Filter based on fraction type The next step in our secchi harmonization is filtering based on the ResultSampleFractionText column. We assign fractions into two categories based on whether the fraction text makes sense or not and then retain only those records that have a fraction with \"Makes sense\". Fractions included in this are NA, \"Total\", \" \", \"None\", \"Unfiltered\", \"Field\". This process drops 94.02 thousand rows leaving 2.5 million remaining in the dataset. 7.0.10 Filter based on sample method We next filter based on the sample method, or SampleCollectionMethod.MethodName column. As with analytical methods, we accomplish this using an external match table csv file that is joined to the dataset. Methods that are coded \"unlikely\" to be accurate methods are dropped. This process drops 30.75 thousand rows leaving 2.47 million remaining. 7.0.11 Filter based on collection equipment Finally, we examine the collection equipment (SampleCollectionEquipmentName) column to check for equipment that indicates non-secchi measurements. Once again we use an external match table csv file that is joined to the dataset. Equipment types that are coded \"unlikely\" to be accurate to secchi are dropped. This process drops 57.19 thousand rows leaving 2.42 million remaining. "],["tss-harmonization-process.html", "8 TSS harmonization process", " 8 TSS harmonization process 8.0.1 Initial dataset After the pre-harmonization process the tss-only WQP dataset contains 3.32 million rows. 8.0.2 Filter for water media The first step in TSS harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The final row count after this is 3.32 million. 8.0.3 Remove fails and other missing data In this step we filter out records based on indications that they have failed data for some reason. We screen the following columns: ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, ResultMeasureValue, and ResultDetectionConditionText. Examples of text that results in a dropped record includes (but is not limited to): \"fail\", \"suspect\", \"error\", \"beyond accept\", \"interference\", \"questionable\", \"problem\", \"violation\", \"rejected\", \"no data\". Specific target text varies by column. 155.91 thousand rows are removed and 3.17 million rows remain. 8.0.4 Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 3.17 million. 8.0.5 Clean approximate values Step 4 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 3.17 million. 8.0.6 Clean values with “greater than” data Step 5 is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 3.17 million. 8.0.7 Harmonize record units The next step in TSS harmonization is working with the units of the WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into milligrams/L: ResultMeasure.MeasureUnitCode conversion mg/L 1.000e+03 mg/l 1.000e+03 ppm 1.000e+03 ug/l 1.000e+00 ug/L 1.000e+00 mg/m3 1.000e+00 ppb 1.000e+00 mg/cm3 1.000e+06 ug/ml 1.000e+03 mg/ml 1.000e+06 ppt 1.000e-06 umol/L 6.008e+01 g/l 1.000e+06 We also limit values to less than 1000 mg/L to ensure realistic data. 112.7 thousand rows are removed. The final row count after this is 3.05 million. 8.0.8 Filter based on analytical method Our next step is to aggregate TSS analytical methods into groups and then filter out methods that may have been erroneously added, were unclear, or which don’t meet our needs. Methods that were grouped as \"Ambiguous\" or \"Unlikely\" are dropped. This process drops 1535.16 million rows leaving 1.52 million remaining. 8.0.9 Filter based on fraction type The final step in our TSS harmonization is filtering based on the ResultSampleFractionText column. We drop records with the following values in this column: \"Fixed“, \"Volatile\", \"Dissolved\", or \"Acid Soluble\". This process drops 0.15 thousand rows leaving 1.52 million remaining in the final harmonized TSS dataset. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
