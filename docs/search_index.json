[["index.html", "1 AquaMatch data harmonization process", " 1 AquaMatch data harmonization process This bookdown documents the harmonization process for data downloaded from the Water Quality Portal (WQP) used to build the AquaMatch dataset, a data product of AquaSat v2. AquaSat v2 is a forthcoming update to AquaSat (Ross et al., 2019). The overarching purpose of AquaSat V2 is to emphasize the individual parts of the AquaSat pipeline that make-up the matchups between satellite and in-situ measurements. The WQP is a data warehouse for water-related data measured or observed within the United States and US Territories managed by the Environmental Protection Agency (EPA), United States Geological Survey (USGS), and the National Water Quality Monitoring Council (NWQMC). AquaMatch collates water quality measurements and observations with optical and thermal satellite data from lakeSR and riverSR. All data in this workflow are processed as parameter groups, a high-level grouping used in AquaMatch for specific water quality measures found in the WQP. This document first describes selections made while downloading WQP data using the {dataRetrieval} package (De Cicco et al., 2023). It then walks through how the chlorophyll a, dissolved organic carbon (DOC), Secchi disc depth, and total suspended solids (TSS) parameter groups are each prepared (“pre-harmonization”), and then filtered and harmonized. Note: for the purposes of this bookdown, many terms surrounded by backticks are column names from the dataset being referenced. Others are code snippets or data values. For those that are column names, a full list of definitions for columns present in the original WQP dataset can be found in Table 7 here. Data are contributed to the WQP by a wide range of data providers (organizationIdentifier and organizationFormalName) with varying sampling and analysis methods and multiple characteristicNames. characteristicNames are defined by the WQP as: The object, property, or substance which is evaluated or enumerated by either a direct field measurement, a direct field observation, or by laboratory analysis of material collected in the field. Parameter groups are comprised of data from multiple characteristicNames in the WQP. For example, chlorophyll-related characteristicNamesinclude “Chlorophyll a”, “Chlorophyll a, corrected for pheophytin”, “Chlorophyll b”, and “Chlorophyll c” among several others. Only a subset of these characteristicNames are included in our “chlorophyll a” parameter group. As mentioned previously, measurements and observations within the WQP for a given parameter group are often sampled and analyzed using a variety of methods. Some of these methods for sampling and analysis are directly interoperable and others are not. The processing steps described within this bookdown walk through how we harmonize the data for each parameter group while flagging or removing data that may not have enough information associated with it. The product of this is a database of WQP entries with interoperability and reliability in mind. The result of this harmonization procedure is two datasets per parameter group. The first is a harmonized dataset that includes all of the original WQP columns in addition to those introduced in this workflow. The second is the output of all harmonization steps, including aggregation to the mean value where simultaneous records occur. We define simultaneous records as those measured on the same day by the same organizationIndentifier at the same location with the same harmonized depth criteria. We believe that most users will only use this aggregated dataset, but we provide the unaggregated version for those who would like to make different decisions at any point in the tiering, flagging, or aggregation steps. "],["technical-details.html", "2 Technical details", " 2 Technical details AquaSat v2 uses the {targets} workflow management R package (Landau, 2021) to reimagine the original AquaSat codebase. The framework for this workflow is based on code adapted from this USGS pipeline and has been further developed by members of the ROSSyndicate. Technical details on {targets} workflows are available in the {targets} User Manual. {targets} workflows are built upon lists of “targets”, which can be thought of as analytical steps written out in code. This workflow uses a targets list spread across multiple scripts in an effort to facilitate organization of the code. _targets.R serves as a main list of targets and references the other lists of targets, which are defined inside 3_harmonize.R and create_bookdown.R. Note that the downloading and harmonizing scripts for WQP data are spread across two GitHub repositories: AquaMatch_download_WQP and AquaMatch_harmonize_WQP, respectively. We recommend using the latest version of R for the best performance when running the pipeline. Additionally, this pipeline relies on .RDS files, for which a significant vulnerability fix was instituted in version 4.4.0 of R. Details about this vulnerability are documented in NIST’s National Vulnerability Database. "],["download-process.html", "3 Download process", " 3 Download process 3.0.1 Catalogue existing data The download process begins by using the {dataRetrieval} package to catalogue the data that is available in the WQP. To do this the user selects parameters of interest and their corresponding characteristicName for retrieving data from the WQP. Below are the current parameters and their characteristicNames as defined in the configuration YAML, 1_inventory/cfg/wqp_codes.yml. Data for these parameters are requested from the WQP within a spatial grid that is mapped in the next section. Code names_yaml &lt;- names(yaml_contents) map_df(.x = names_yaml, .f = ~yaml_contents[.x] %&gt;% as_tibble() %&gt;% rename(characteristicName = 1) %&gt;% mutate(parameter = .x) %&gt;% select(parameter, characteristicName)) %&gt;% kable() %&gt;% kable_material() %&gt;% collapse_rows(columns = 1, valign = &quot;top&quot;) parameter characteristicName chlorophyll Chlorophyll a Chlorophyll a (probe relative fluorescence) Chlorophyll a, corrected for pheophytin Chlorophyll a (probe) Chlorophyll a, free of pheophytin Chlorophyll a, uncorrected for pheophytin Chlorophyll a - Phytoplankton (suspended) doc Organic carbon Total carbon Non-purgeable Organic Carbon (NPOC) sdd Depth, Secchi disk depth Depth, Secchi disk depth (choice list) Secchi Reading Condition (choice list) Secchi depth Water transparency, Secchi disc 3.0.2 Maps of data spread The pipeline uses a spatial grid to download data from the WQP in batches. Maps are presented below with counts of records across this spatial grid. Note: The counts here are for raw data that are not yet filtered or harmonized. Code # Combine sample counts in each grid_id with the grid polygons grid_counts &lt;- left_join(x = global_grid, y = site_counts %&gt;% count(grid_id), by = c(&quot;id&quot; = &quot;grid_id&quot;)) %&gt;% st_transform(crs = 9311) state_selection &lt;- states(progress_bar = FALSE) %&gt;% st_transform(crs = 9311) ## Retrieving data for the year 2021 Code # Conterminous US map: conterminous_us &lt;- state_selection %&gt;% filter(!(NAME %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;American Samoa&quot;, &quot;Guam&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;))) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = conterminous_us, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(conterminous_us)[,&quot;X&quot;]), max(st_coordinates(conterminous_us)[,&quot;X&quot;])), ylim = c(min(st_coordinates(conterminous_us)[,&quot;Y&quot;]), max(st_coordinates(conterminous_us)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Conterminous US&quot;) + theme_bw() Code # Alaska map: AK &lt;- state_selection %&gt;% filter(NAME == &quot;Alaska&quot;) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = AK, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(AK)[,&quot;X&quot;]), max(st_coordinates(AK)[,&quot;X&quot;])), ylim = c(min(st_coordinates(AK)[,&quot;Y&quot;]), max(st_coordinates(AK)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Alaska&quot;) + theme_bw() Code # Hawaii map: HI &lt;- state_selection %&gt;% filter(NAME == &quot;Hawaii&quot;) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = HI, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(HI)[,&quot;X&quot;]), 0.9 * max(st_coordinates(HI)[,&quot;X&quot;])), ylim = c(1.1 * min(st_coordinates(HI)[,&quot;Y&quot;]), max(st_coordinates(HI)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Hawaii&quot;) + theme_bw() Code # American Samoa map: AS &lt;- state_selection %&gt;% filter(NAME %in% c(&quot;American Samoa&quot;)) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = AS, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(1.025 * min(st_coordinates(AS)[,&quot;X&quot;]), 0.975 *max(st_coordinates(AS)[,&quot;X&quot;])), ylim = c(1.05 * min(st_coordinates(AS)[,&quot;Y&quot;]), max(st_coordinates(AS)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;American Samoa&quot;) + theme_bw() Code # Guam &amp; Commonwealth of the Northern Mariana Islands map GU_CNMI &lt;- state_selection %&gt;% filter(NAME %in% c(&quot;Guam&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;)) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = GU_CNMI, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(1.025 * min(st_coordinates(GU_CNMI)[,&quot;X&quot;]), 0.975 * max(st_coordinates(GU_CNMI)[,&quot;X&quot;])), ylim = c(0.95 * min(st_coordinates(GU_CNMI)[,&quot;Y&quot;]), 1.05 * max(st_coordinates(GU_CNMI)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Guam &amp; Commonwealth of the Northern Mariana Islands&quot;) + theme_bw() Code # Puerto Rico &amp; United States Virgin Islands map: PR_VI &lt;- state_selection %&gt;% filter(NAME %in% c(&quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;)) ggplot() + geom_sf(data = grid_counts, aes(fill = n)) + geom_sf(data = PR_VI, fill = NA, color = &quot;black&quot;) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(0.95 * min(st_coordinates(PR_VI)[,&quot;X&quot;]), 1.05 * max(st_coordinates(PR_VI)[,&quot;X&quot;])), ylim = c(1.075 * min(st_coordinates(PR_VI)[,&quot;Y&quot;]), 0.925 * max(st_coordinates(PR_VI)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;, trans = &quot;log10&quot;, labels = scales::label_number(), na.value = &quot;white&quot;, breaks = c(1, 10, 100, 1000, 10000)) + ggtitle(&quot;Puerto Rico &amp; United States Virgin Islands&quot;) + theme_bw() "],["tiering-flagging-and-quality-control-philosophy.html", "4 Tiering, flagging, and quality control philosophy 4.1 Handling NA values 4.2 Heterogenous data 4.3 Simultaneous records", " 4 Tiering, flagging, and quality control philosophy The variety of data providers, parameters, and methods in the WQP inevitably results in a heterogenous dataset that requires rigorous quality control before analytical use. In this chapter, we detail the general tiering, flagging, and quality control philosophy we apply across all parameter groups. 4.1 Handling NA values Columns related directly to the interoperability of data in the WQP, specifically field and lab methodology and depth-related columns, often contain many NAs in part due to inconsistent entry across data providers. A highly restrictive filtering of the WQP that requires completed data fields would result in very limited data, in part, due to the prevalence of these NA values. Therefore, we built this pipeline to resolve as many NAs as possible, but to also include NAs within an “inclusive” data tier if all columns of interest are not resolvable. Specifically, we retain NA values if removing those NA records would drop 10% or more of the available data. 4.2 Heterogenous data In addition to NA values, WQP entries can be heterogenous for reasons such as: Having a variety of analytical methods with different levels of precision and accuracy that are used to quantify the same parameter Having a variety of sampling methods used to acquire the same parameter with varying levels of interoperability Having a variety of names or descriptions that represent the same analytical or sampling method across different organizations Having information relevant to assessing data reliability or method choice spread across multiple columns Containing observations with sample collection methods and analytical methods that are incompatible Having data with differing levels of meta data In order to control for some of this variation and provide end-users with something more readily navigable we have incorporated several tiers and flagging systems into the AquaSat v2 dataset: Method tiering: Tiers indicating the veracity of data based on the method used to acquire measurements, thereby allowing for the identification of self-similar samples Field flags: Flags typically indicating whether the sample collection method is consistent with the analytical method. When there is no analytical aspect to the parameter (e.g., secchi disk depth) this column provides additional information that may be helpful for interpretation of the value given any field comments or methodological differences in data collection reported in the WQP entry. Given these differences, flag values and their meaning may differ by parameter and as a result, flag values will be explained in the documentation for each parameter Depth flags: Flags indicating the type of water sampling depth measurement or completeness of the measurement Miscellaneous flags: Flags that will vary in meaning and use by parameter. Some variables will not use them 4.2.1 Method tiering The ResultAnalyticalMethod.MethodName column from the WQP is often the primary column we use for determining which tier each record falls within. Details on how each parameter’s methods were sorted into tiers can be found in its corresponding harmonization chapter. The primary purpose of our tiering is to determine the reliability and accuracy of each analytical method across data providers and throughout time for each parameter in the AquaSat v2 dataset. We developed the following categories, which are represented in the tier column of the final dataset: Tier 0: Restrictive. Data that are verifiably self-similar across organizations and time-periods and can be considered highly reliable Tier 1: Narrowed. Data that we have good reason to believe are self-similar, but for which we can’t verify full compatibility across data providers Tier 2: Inclusive. Data that are assumed to be reliable and are harmonized to our best ability given the information available from the data provider. This tier includes NA or non-resolvable descriptions for the analytical method, which often make up the majority of method descriptions for any given parameter. Because this tier represents many analytical methods, direct compatibility between samples is not guaranteed. 4.2.2 Field flags The field_flag column is used to check if the sample collection method (SampleCollectionMethod.MethodName) is reasonable (flag = 0), suspect (flag = 1), or inconclusive (flag = 2). “Reasonable” flags mean that the sample collection or observation method and the analytical method are in harmony with one another. “Suspect” flags are assigned when the sample collection method description isn’t consistent with what would be expected given the characteristicName and the ResultAnalyticalMethod.MethodName for a given record. “Inconclusive” or unknown flags are assigned to records with “inclusive” tiers because these tiers include values such as NA, where it’s typically impossible to determine the appropriateness of a collection method for the sample record. The relationship between field flags and tiers will vary by parameter; thus, each harmonization chapter contains specific information for these different cases. 4.2.3 Depth flags There are four columns that explicitly contain depth information for a given WQP entry, all of which contain a variety of measurement units. Below is a list of the four columns and their definitions according to the {dataRetrieval} package documentation. ActivityDepthHeightMeasure.MeasureValue: “A measurement of the vertical location (measured from a reference point) at which an activity occurred.” ResultDepthHeightMeasure.MeasureValue: “A measurement of the vertical location (measured from a reference point) at which a result occurred.” Only in STORET ActivityTopDepthHeightMeasure.MeasureValue: “A measurement of the upper vertical location of a vertical location range (measured from a reference point) at which an activity occurred.” ActivityBottomDepthHeightMeasure.MeasureValue: “A measurement of the lower vertical location of a vertical location range (measured from a reference point) at which an activity occurred.” Each of the above columns has a corresponding column containing the the associated unit used in measuring the item: ActivityDepthHeightMeasure.MeasureUnitCode ResultDepthHeightMeasure.MeasureUnitCode ActivityTopDepthHeightMeasure.MeasureUnitCode ActivityBottomDepthHeightMeasure.MeasureUnitCode 4.2.3.1 Pre-processing Prior to assigning the depth_flag we complete a few pre-processing steps: Convert the following character values to an explicit NA: “NA”, “999”, “-999”, “9999”, “-9999”, “-99”, “99”, “NaN” Convert depths in all four columns to meters from the depth unit listed by the data provider Create a single “discrete” sample depth column using a combination of the ActivityDepthHeightMeasure.MeasureValue and ResultDepthHeightMeasure.MeasureValue columns. We use ActivityDepth value when ResultDepth value is missing, and ResultDepth when ActivityDepth is missing. If both columns have values but disagree we use an average of the two. This pre-processing results in three “harmonized” columns reporting water sampling depth values in meters: harmonized_discrete_depth_value, harmonized_top_depth_value, harmonized_bottom_depth_value. Sample depth flags are assigned using the harmonized depth columns that result from the pre-processing steps above. If the record has no depth listed it is assigned a depth_flag of 0. A record with only discrete depth listed in the harmonized_discrete_depth_value is given a depth_flag of 1. A record with top and/or bottom depth (harmonized_top_depth_value, harmonized_bottom_depth_value), indicating an integrated sample, is assigned a depth_flag of 2, and any combination of discrete + top and/or bottom depths is assigned a depth_flag of 3, since the sample depth(s) can not be reconciled with certainty. 4.2.4 Miscellaneous flags The misc_flag column is included as a flexible flag column in order to note important information that isn’t covered by the tiering and flags defined above. Some parameters, like chlorophyll a, will not use this column at all and will therefore just contain NA values in places of flags. Values and their meaning will differ by parameter and as a result, flag values will be explained in the documentation for each parameter. 4.2.5 Time and date handling Time and date data are provided inconsistently in the WQP, so we have a standardized process for harmonizing these data to provide AquaSat users a consistent date-time column across all parameters. We have designed a function, fill_date_time(), to create harmonized date and time columns before the broader data harmonization process. It adds the columns harmonized_tz (time zone as GMT offset), harmonized_local_time (local date time relative to GMT offset), and harmonized_utc (date time reported as time in UTC) to the dataset. The function first transforms site-specific data into a unified geographic coordinate reference system and then looks up the local time zone for each monitoring site in the dataset using latitude and longitude with the lutz::tz_lookup() function. This spatially-acquired time zone data is temporarily stored in the fetched_tz column. The harmonized_tz column is then generated using any timezones provided in ActivityStartTime.TimeZoneCode, while filling in gaps in timezone data with those identified using fetched_tz. We then create the harmonized_local_time column by combining ActivityStartDate and ActivityStartTime.Time. If ActivityStartTime.Time is NA or \"00:00:00\" we instead use \"11:59:59\" in harmonized_local_time. Very few, if any, records contain this exact time stamp, so it can be used as a proxy to filter or flag “synthetic” sampling times if desired by the end user. Note that when we insert \"11:59:59\" and the ActivityStartTime.TimeZoneCode was provided in UTC or GMT we treat \"11:59:59\" as local time and use fetched_tz for harmonized_tz. But if the ActivityStartTime.Time was not “synthetic” we convert the UTC/GMT-based time into the local time determined by fetched_tz. At this point the harmonized_utc column is created by converting the contents of harmonized_local_time to UTC. Next, the harmonized_tz column needs one final conversion: Up to this point its contents have been a mix of location-based strings like \"America/Chicago\" or short codes like \"CST\". We translate both formats into a unified GMT offset format where, e.g., \"CDT\" would be \"Etc/GMT+5\" and \"CST\" would be \"Etc/GMT+6\". Lastly, harmonized_local_time is formatted as a string with no time zone stamp. Note: ActivityStartDateTime is a UTC datetime column that is added by the {dataRetrieval} package. In most, but not all, cases it will agree with harmonized_utc. Differences in some values occur because 1) ActivityStartDateTime is NA for ActivityStartTime.TimeZoneCode values of NA, \"AST\", \"ADT\", \"GST\", \"IDLE\"; or 2) we handle \"00:00:00\" values of ActivityStartTime.Time the same as NAs whereas ActivityStartDateTime does not. 4.2.6 Anonymization Occasionally, personally identifiable information is included in comment fields in the WQP. We use a function, anonymize_text(), to find and obscure phone numbers and email addresses. By default this is applied to the ActivityCommentText, ResultLaboratoryCommentText, and ResultCommentText columns and occurs after downloading the initial raw WQP data. 4.2.7 General notes Generally speaking, we avoid further classification for any WQP entry’s parameter methodology, tier, or flag unless there are at least 1% of observations with the same unresolved text. When building the tier and _flag columns we use temporary columns with _tag in the name to track important qualities that inform our tier and flag decisions. These columns are not exported to the final product and are intended as open-ended tools for tracking records, so they will vary greatly between each parameter’s codebase. 4.3 Simultaneous records The WQP contains records that can be considered true duplicates and others where multiple non-identical measurements are recorded at the same time, place, and depth by the same organization. We deal with these within each parameter’s harmonization step by taking the mean value from simultaneous observations. We also report the coefficient of variation and the total number of entries contributing to the mean in the harmonized_value_cv and harmonized_row_count columns of the final dataset. This record aggregation is dealt with separately for each parameter so that specific accommodations can be made based on their tiering and value cleaning processes. Additionally, this step also requires us to group the dataset by a subset of its original columns, necessarily resulting in a reduced subset of columns in the final data product. To aid in back-joining for advanced AquaSat users, we provide a subgroup_id column identifying the grouping used to create the aggregated mean and coefficient of variation values. The subgroup_id is also present in an upstream, pre-aggregated dataset version that contains all records prior to aggregation and the original WQP columns along with the columns created in the harmonization steps. The upstream, pre-aggregated dataset is the p3_chla_preagg_grouped pipeline target. "],["chlorophyll-a-chla-harmonization-process.html", "5 Chlorophyll a (chla) harmonization process 5.1 Pre-harmonization of the raw chlorophyll WQP dataset 5.2 Harmonization-ready chlorophyll dataset", " 5 Chlorophyll a (chla) harmonization process Following the completion of the {dataRetrieval} download process described previously, the pipeline contains raw WQP data for each parameter of interest. Before we harmonize each parameter we run through a series of universal “pre-harmonization” steps, which ensure that the datasets are appropriately formatted when entering their harmonization routines. The text below first walks through the pre-harmonization steps for the raw chlorophyll a dataset and then delves into the specifics of the harmonization process. 5.1 Pre-harmonization of the raw chlorophyll WQP dataset At the start of the pre-harmonization process the raw chlorophyll WQP dataset contains 4.7 million rows. 5.1.1 Missing results Next, records that have missing data are dropped from the dataset. Several criteria are used when checking for missing data. If any of the below criteria are met the row is flagged as missing: Both the result column and detection limit column had NA data Result, result unit, activity comment, laboratory comment, and result comment columns are all NA The result comment column contains any user-provided text indicating a missing value, currently including: analysis lost, not analyzed, not recorded, not collected, or no measurement taken 50.7 thousand rows are dropped, resulting in a final count of 4.6 million. 5.1.2 Filter status The final step in pre-harmonization is to filter the ResultStatusIdentifier column to include only the following statuses: \"Accepted\" \"Final\" \"Historical\" \"Validated\" \"Preliminary\" NA These statuses generally indicate a reliable result having been reached, however we also include NA in an effort to be conservative. More specifically, when making decisions for this and other columns we occasionally retain NA values if removing the records would otherwise drop 10% or more of the available data. This step removes 40.2 thousand rows of data, leaving it with 4.6 million rows remaining. 5.2 Harmonization-ready chlorophyll dataset Once ready for harmonization, the chlorophyll a-only WQP dataset contains the following user-defined characteristicNames: Chlorophyll a, Chlorophyll a (probe relative fluorescence), Chlorophyll a, corrected for pheophytin, Chlorophyll a (probe), Chlorophyll a, free of pheophytin, Chlorophyll a, uncorrected for pheophytin, Chlorophyll a - Phytoplankton (suspended). These names are chosen in order to select for only those measurements that pertain to chlorophyll a. 5.2.1 Filter for water media We next ensure that the media type for all chlorophyll data is \"Surface Water\", \"Water\", \"Estuary\", or NA. 2886 rows are removed. The final row count after this is 4.6 million. 5.2.2 Document and remove fails In this step we filter out records based on indications that they have failed data quality assurance or quality control for some reason given by the data provider (these instances are referred to here as “failures”). After reviewing the contents of the ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, and ResultMeasureValue_original columns, we developed a list of terms that captured the majority of instances where records had failures or unacceptable measurements. We found the phrasing to be consistent across columns and so we searched for the same (case agnostic) terms in all four locations. The terms are: “beyond accept”, “cancelled”, “contaminat”, “error”, “fail”, “improper”, “instrument down”, “interference”, “invalid”, “no result”, “no test”, “not accept”, “outside of accept”, “problem”, “QC EXCEEDED”, “questionable”, “suspect”, “unable”, “violation”, “reject”, “no data”. Below are pie charts that break down the number of failure detections by column. Note that the plotting below is automated so if one or more of the columns listed above are not plotted, this indicates that the column(s) did not return any matches for the failure phrases. Also note that a single record can contain multiple failure phrases; therefore, failure phrases are not mutually exclusive. 5.2.2.1 ActivityCommentText fail detects 5.2.2.2 ResultCommentText fail detects 5.2.2.3 ResultLaboratoryCommentText fail detects 5.2.2.4 ResultCommentText fail detects 89 thousand rows are removed after detecting failure-related phrases and 4.5 million rows remain. 5.2.3 Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. We produce a new column, mdl_flag, from the MDL cleaning process. Records where no MDL-based adjustment was made and which are at or above the MDL are assigned a 0. Records with corrected values based on the MDL method are assigned a 1. Finally, records where no MDL-based adjustment was made and which contain a numeric value below the provided MDL are assigned a 2. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 4.5 million. 5.2.4 Clean approximate values Cleaning approximate values involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. Records with corrected values based on the above method are noted with a 1 in the approx_flag column. Note, however, that occasionally approximate language will be used in a record but not changed or flagged. This occurs when the language is used in a comment-related column and not the result column itself, meaning that there is a usable numeric value provided (and thus doesn’t need correction). This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 4.5 million. 5.2.5 Clean values with “greater than” data The next step is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. Records with corrected values based on the above method are noted with a 1 in the greater_flag column. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 4.5 million. 5.2.6 Drop unresolved NA measurements The goal of the preceding three steps was to prevent records with seemingly missing measurement data from being dropped if there was still a chance of recovering a usable value. At this point we’ve finished with that process and we proceed to check for remaining records with NA values in their harmonized_value column. If they exist, they are dropped. We also filter out any negative values in the dataset at this point. 26.1 thousand rows are removed. The final row count after this is 4.5 million. 5.2.7 Harmonize record units The next step in chla harmonization is converting the units of WQP records. We create the following conversion table, which is used to translate units provided in WQP into micrograms per liter (ug/L): ResultMeasure.MeasureUnitCode conversion mg/l 1e+03 mg/L 1e+03 ppm 1e+03 ug/l 1e+00 ug/L 1e+00 mg/m3 1e+00 ppb 1e+00 mg/cm3 1e+06 ug/ml 1e+03 mg/ml 1e+06 ppt 1e+06 ug/mL 1e+03 mg/mL 1e+06 Below is a pie chart that breaks down the different unit codes that were dropped in the unit harmonization process, and how many records were lost with each code. Note that while RFU is a valid unit for chlorophyll-a from an in situ probe, we have no way to convert those data to ug/L, and therefore dismiss those measurements from the dataset. Additionally we provide a set of histograms of harmonized measurements by CharacteristicName: 242.7 thousand rows are removed. The final row count after this is 4.2 million. 5.2.8 Clean depth data The next harmonization step cleans the four depth-related columns obtained from the WQP. The details behind this step are covered in the Depth flags section of the Tiering, flagging, and quality control philosophy chapter. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 4.2 million. 5.2.9 Filter and tier methods We next review the analytical methods used in measuring chlorophyll a, primarily by classifying the text provided with each record in ResultAnalyticalMethod.MethodName. Once these methods are classified we arrange them into hierarchical tiers as described in the Method tiering section of the Tiering, flagging, and quality control philosophy chapter. However, prior to classification we check the ResultAnalyticalMethod.MethodName column for names that indicate non-chlorophyll a measurements. Phrases used to flag and remove unrelated methods from chlorophyll a data are: “sulfate”, “sediment”, “5310”, “counting”, “plasma”, “turbidity”, “coliform”, “carbon”, “2540”, “conductance”, “nitrate”, “nitrite”, “nitrogen”, “alkalin”, “zooplankton”, “phosphorus”, “periphyton”, “peri”, “biomass”, “temperature”, “elemental analyzer”, “2320”. This process drops 69.2 thousand rows leaving 4.2 million remaining. The next step towards creating tiers is to then classify the methods in ResultAnalyticalMethod.MethodName into either: HPLC methods, spectrophotometer and fluorometer methods, or methods for which a pheophytin correction is recorded as part of the methodology. These classifications are not the final tiers, but they inform the tiering in the final step of this process. The criteria for each of the above classifications are: HPLC: Detection of “447”, “chromatography”, or “hplc” in the ResultAnalyticalMethod.MethodName or presence of 70951 or 70953 in the USGSPCode column Spectro/fluoro: Detection of “445”, “fluor”, “Welshmeyer”, “fld”, “10200”, “446”, “trichromatic”, “spectrophoto”, “monochrom”, “monchrom”, or “spec” not as part of a word in ResultAnalyticalMethod.MethodName Pheophytin correction: Detection of “correct”, “445”, “446”, or “in presence” in ResultAnalyticalMethod.MethodName or detection of “corrected for pheophytin” or “free of pheophytin” in CharacteristicName Finally, we group the data into three tiers as described in Tiering, flagging, and quality control philosophy. These tiers are: Tier Name Description Chl a details 0 Restrictive Data that are verifiably self-similar across organizations and time-periods and can be considered highly reliable and interoperable Includes records using HPLC methods 1 Narrowed Data that we have good reason to believe are self-similar, but for which we can’t verify full interoperability across data providers Spectrophotometer and fluorometer methods that are also pheophytin-corrected OR records where USGSPCode is 32209 2 Inclusive Data that are assumed to be reliable and are harmonized to our best ability given the information available from the data provider. This tier includes NA or non-resolvable descriptions for the analytical method, which often make up the majority of methods descriptions for any given parameter All other records by default, including NA methods and in situ probes Note: Spectrophotometer and fluorometer methods that are labeled as pheophytin-corrected are grouped into the “Narrowed” tier. Depending on the exact implementation of EPA method 445, the correction philosophy may vary, and there is no agreed upon method to rectify inconsistencies in data entry related to these methodological differences. The final harmonization product that aggregates simultaneous records does not retain the CharacteristicName or ResultAnalyticalMethod.MethodName columns. However, the non-aggregated version of the harmonized dataset (p3_chla_preagg_grouped) includes those columns. We encourage any users that want to explicitly compare or account for pheophytin correction to use the p3_chla_preagg_grouped dataset. At this point we export a file (3_harmonize/out/chla_tiering_record.csv) that contains a record of how specific method text was tiered and how many row counts corresponded to each method. 5.2.10 Flag based on field methods Next we flag field sampling methods based primarily on the SampleCollectionMethod.MethodName column. We first classify each record into either in vitro or in situ methods (i.e., in vitro assumes a water sample was collected and taken to a lab for analysis; in situ assumes a measurement was obtained in the field). We used the following strings to mark in vitro samples: “grab”, “bottle”, “vessel”, “bucket”, “jar”, “composite”, “integrate”, “UHL001”, “surface”, “filter”, “filtrat”, “1060B”, “kemmerer”, “collect”, “rosette”, “equal width”, “vertical”, “van dorn”, “bail”, “sample”, “sampling”, “lab” not in the middle of another word, or a “G” on its own as shorthand for “grab”. In situ samples were detected using “in situ”, “probe”, or “ctd”. Lastly we created the field flag based on whether the sampling method used agrees with the analytical method. Flags of 0 indicated that the field sampling method is in agreement with the analytical method, 1 indicates that the field sampling methods are uncharacteristic of the analytical method, and anything with tier of 2 is given a field flag of 2 due to the ambiguity associated with those observations’ analytical methods and corresponding sampling methods. The following rules are used for chlorophyll a field sampling flags: Flag 0: Restrictive and narrowed tiers with in vitro field methods Flag 1: Restrictive and narrowed tiers with in situ field methods Flag 2: Any entry in the inclusive tier No records should be removed by this process and so there are 0 rows dropped leaving 4.2 million remaining in the harmonized chlorophyll a dataset. 5.2.11 Miscellaneous flag Next we add a placeholder for the miscellaneous flag column, misc_flag. Some parameters will have additional flagging requirements that chlorophyll a does not, so we include this placeholder to maintain the same columns across all parameter data products. No records should be removed by this process and so there are 0 rows dropped leaving 4.2 million remaining in the harmonized chlorophyll a dataset. 5.2.12 Remove unrealistic values Before finalizing the dataset we remove chlorophyll a values that are beyond a realistic threshold. We use 1000 ug/L as our cutoff for removal (Wetzel, 2001, Chapter 15, Figure 19). 7662 rows are removed. The final row count after this is 4.2 million. 5.2.13 Aggregate simultaneous records The final step of chlorophyll a harmonization is to aggregate simultaneous observations. Any group of samples determined to be simultaneous are simplified into a single record containing the mean and coefficient of variation (CV) of the group. These can be either true duplicate entries in the WQP or records with non-identical values recorded at the same time and place and by the same organization (field and/or lab replicates/duplicates). The CV can be used to filter the dataset based on the amount of variability that is tolerable to specific use cases. Note, however, that many entries will have a CV that is NA because there are no simultaneous records or 0 because the records are duplicates and all entries have the same harmonized_value. We identify simultaneous records to aggregate by creating identical subgroups (subgroup_id) from the following columns: parameter, OrganizationIdentifier, MonitoringLocationIdentifier, MonitoringLocationTypeName, ResolvedMonitoringLocationTypeName, ActivityStartDate, ActivityStartDateTime, ActivityStartTime.TimeZoneCode, harmonized_tz, harmonized_utc, harmonized_top_depth_value, harmonized_top_depth_unit, harmonized_bottom_depth_value, harmonized_bottom_depth_unit, harmonized_discrete_depth_value, harmonized_discrete_depth_unit, depth_flag, mdl_flag, approx_flag, greater_flag, tier, field_flag, misc_flag, harmonized_units. This selection limits the columns included in the final dataset, but we also provide a copy of the AquaMatch dataset prior to its aggregation (pipeline target p3_chla_preagg_grouped), and include the subgroup_id column, so that users can use the non-aggregated data as well and match make joins between dataset versions. The final, aggregated values are presented in the harmonized_value and harmonized_value_cv columns. The number of rows used per group is recorded in the harmonized_row_count column. 0.8 million rows dropped leaving 3.4 million remaining in the final harmonized and aggregated chlorophyll a dataset. 5.2.14 Harmonized chlorophyll a At this point the harmonization of the chlorophyll a data from the WQP is complete and we export the final dataset for use later in the workflow. Below is a set of histograms showing the distribution of harmonized measurements (top) and CVs (bottom) broken down by tier after aggregating simultaneous records. Lastly, a series of maps showing the geographic distribution of records by tier: 5.2.14.1 Conterminous US 5.2.14.2 Alaska 5.2.14.3 Hawaii 5.2.14.4 American Samoa 5.2.14.5 Guam and the Commonwealth of the Northern Mariana Islands 5.2.14.6 Puerto Rico and the United States Virgin Islands "],["doc-harmonization-process.html", "6 DOC harmonization process 6.1 Pre-harmonization of the raw doc WQP dataset 6.2 Harmonization-ready DOC dataset", " 6 DOC harmonization process Following the completion of the {dataRetrieval} download process described previously, the pipeline contains raw WQP data for each parameter of interest. Before we harmonize each parameter we run through a series of universal “pre-harmonization” steps, which ensure that the datasets are appropriately formatted when entering their harmonization routines. The text below first walks through the pre-harmonization steps for the raw dissolved organic carbon (DOC) dataset and then delves into the specifics of the harmonization process. 6.1 Pre-harmonization of the raw doc WQP dataset At the start of the pre-harmonization process the raw DOC WQP dataset contains 2.7 million rows. 6.1.1 Missing results Next, records that have missing data are dropped from the dataset. Several criteria are used when checking for missing data. If any of the below criteria are met the row is flagged as missing: Both the result column and detection limit column had NA data Result, result unit, activity comment, laboratory comment, and result comment columns are all NA The result comment column contains any user-provided text indicating a missing value, currently including: analysis lost, not analyzed, not recorded, not collected, or no measurement taken 94.7 thousand rows are dropped, resulting in a final count of 2.7 million. 6.1.2 Filter status The final step in pre-harmonization is to filter the ResultStatusIdentifier column to include only the following statuses: \"Accepted\" \"Final\" \"Historical\" \"Validated\" \"Preliminary\" NA These statuses generally indicate a reliable result having been reached, however we also include NA in an effort to be conservative. More specifically, when making decisions for this and other columns we occasionally retain NA values if removing the records would otherwise drop 10% or more of the available data. This step removes 2037 rows of data, leaving it with 2.6 million rows remaining. 6.2 Harmonization-ready DOC dataset Once ready for harmonization, the DOC WQP dataset contains the following user-defined characteristicNames: Organic carbon, Total carbon, Non-purgeable Organic Carbon (NPOC). These names are chosen in order to select for only those measurements that pertain to DOC. 6.2.1 Filter media and fractions We next ensure that the media type for all DOC data is \"Surface Water\", \"Water\", \"Estuary\", or NA. Additionally at this step we select a subset of CharacteristicName and ResultSampleFractionText combinations that are relevant to DOC. These are: CharacteristicName ResultSampleFractionText Non-purgeable Organic Carbon (NPOC) Dissolved Non-purgeable Organic Carbon (NPOC) Filtered, lab Non-purgeable Organic Carbon (NPOC) None Non-purgeable Organic Carbon (NPOC) Total Non-purgeable Organic Carbon (NPOC) NA Organic carbon Dissolved Organic carbon Filtered, lab Organic carbon Filtered, field Organic carbon Filterable Organic carbon Total Organic carbon Organic Total carbon Filterable Total carbon NA Total carbon Total Total carbon Organic Below are pie charts that break down the type of fractions dropped for each CharacteristicName, including record counts: 6.2.1.1 Non-purgeable organic carbon (NPOC) fractions dropped 6.2.1.2 Organic carbon fractions dropped 6.2.1.3 Total carbon fractions dropped 303 thousand rows are removed. The final row count after this is 2.3 million. 6.2.2 Document and remove fails In this step we filter out records based on indications that they have failed data quality assurance or quality control for some reason given by the data provider (these instances are referred to here as “failures”). After reviewing the contents of the ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue_original columns, we developed a list of terms that captured the majority of instances where records had failures or unacceptable measurements. (Note: ResultMeasureValue_original is a duplicate, character version of the ResultMeasureValue column that we use as a reference for the column’s contents before it was converted to a numeric type.) We found the phrasing to be consistent across columns and so we searched for the same (case agnostic) terms in all four locations. The terms are: “beyond accept”, “cancelled”, “contaminat”, “error”, “fail”, “improper”, “interference”, “invalid”, “no result”, “no test”, “not accept”, “outside of accept”, “problem”, “questionable”, “suspect”, “unable”, “violation”, “reject”, “no data”, “time exceed”, “value extrapolated”, “exceeds”, “biased”, “parameter not required”, “not visited”. Below are pie charts that break down the number of failure detections by column. Note that the plotting below is automated so if one or more of the columns listed above are not plotted, this indicates that the column(s) did not return any matches for the failure phrases. Also note that a single record can contain multiple failure phrases; therefore, failure phrases are not mutually exclusive. 6.2.2.1 ActivityCommentText fail detects 6.2.2.2 ResultCommentText fail detects 6.2.2.3 ResultLaboratoryCommentText fail detects 6.2.2.4 ResultDetectionConditionText fail detects 6.2.2.5 ResultMeasureValue_original fail detects 58.5 thousand rows are removed after detecting failure-related phrases and 2.3 million rows remain. 6.2.3 Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. We produce a new column, mdl_flag, from the MDL cleaning process. Records where no MDL-based adjustment was made and which are at or above the MDL are assigned a 0. Records with corrected values based on the MDL method are assigned a 1. Finally, records where no MDL-based adjustment was made and which contain a numeric value below the provided MDL are assigned a 2. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.3 million. 6.2.4 Clean approximate values Cleaning approximate values involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. Records with corrected values based on the above method are noted with a 1 in the approx_flag column. Note, however, that occasionally approximate language will be used in a record but not changed or flagged. This occurs when the language is used in a comment-related column and not the result column itself, meaning that there is a usable numeric value provided (and thus doesn’t need correction). This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.3 million. 6.2.5 Clean values with “greater than” data The next step is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. Records with corrected values based on the above method are noted with a 1 in the greater_flag column. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.3 million. 6.2.6 Drop unresolved NA measurements The goal of the preceding three steps was to prevent records with seemingly missing measurement data from being dropped if there was still a chance of recovering a usable value. At this point we’ve finished with that process and we proceed to check for remaining records with NA values in their harmonized_value column. If they exist, they are dropped. We also filter out any negative values in the dataset at this point. 3920 rows are removed. The final row count after this is 2.3 million. 6.2.7 Harmonize record units The next step in doc harmonization is converting the units of WQP records. We create the following conversion table, which is used to translate units provided in WQP into mg per liter (mg/L): ResultMeasure.MeasureUnitCode conversion mg/L 1.00000e+00 mg/l 1.00000e+00 ppm 1.00000e+00 ug/l 1.00000e-03 ug/L 1.00000e-03 mg/m3 1.00000e-03 ppb 1.00000e-03 mg/cm3 1.00000e+03 ug/ml 1.00000e+00 ug/mL 1.00000e+00 mg/ml 1.00000e+03 ppt 1.00000e+03 umol/L 1.20107e-02 Below is a pie chart that breaks down the different unit codes that were dropped in the unit harmonization process, and how many records were lost with each code. Additionally we provide a set of histograms of harmonized measurements by CharacteristicName: 8.2 thousand rows are removed. The final row count after this is 2.3 million. 6.2.8 Clean depth data The next harmonization step cleans the four depth-related columns obtained from the WQP. The details behind this step are covered in the Depth flags section of the Tiering, flagging, and quality control philosophy chapter. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.3 million. 6.2.9 Filter and tier methods We next review the analytical methods used in measuring DOC, primarily by classifying the text provided with each record in ResultAnalyticalMethod.MethodName. Once these methods are classified we arrange them into hierarchical tiers as described in the Method tiering section of the Tiering, flagging, and quality control philosophy chapter. However, prior to classification we check the ResultAnalyticalMethod.MethodName column for names that indicate non-DOC measurements. Phrases used to flag and remove unrelated methods from DOC data are: “Oxygen”, “Nitrogen”, “Ammonia”, “Metals”, “E. coli”, “Anion”, “Cation”, “Phosphorus”, “Silica”, “PH”, “HARDNESS”, “Nutrient”, “Turbidity”, “Nitrate”, “Conductance”, “Alkalinity”, “Chlorophyll”, “Solids”, “Temperature”, “Color in Water”, “Coliform”, “PARTICULATE CARBON (inorg+org)”, “5210”, “bed sed”, “bs, calc”, “5220”, “Suspended-Sediment in Water”. This process drops 82.9 thousand rows leaving 2.2 million remaining. The next step towards creating tiers is to then classify the methods in ResultAnalyticalMethod.MethodName into either: Combustion + infrared methods, Persulfate-UV/Heated Persulfate Oxidation+IR detection methods, Wet Oxidation+Persulfate+IR methods, Elemental Analyzer methods, or Carbonaceous Analyzer methods. These classifications are not the final tiers, but they inform the tiering in the final step of this process. The criteria for each of the above classifications are: Combustion+IR: Detection of “5310 B”, “5310B”, “415.1”, “combustion” but only without “5310 C” or “5310C”, “0.45u silver, persulfate IR”, “0.7 um GFF, combust IR”, “Laboratory Procedures for Water Quality Chemical Analysis”, “CO2 formation”, “Qian &amp; Mopper 1996”, or “Shimadzu TOC Analyzer” in ResultAnalyticalMethod.MethodName Persulfate-UV/Heated Persulfate Oxidation+IR: Detection of “5310 C”, “5310C”, “persulfate oxid”, “415.2”, “Ultraviolet”, “Heated-Persulfate”, or “UV” in ResultAnalyticalMethod.MethodName Wet Oxidation+Persulfate+IR detection: Detection of “0.45 ?um cap”, “0.45um cap”, “wet oxidation”, “5310 D”, “5310D”, “415.3”, “O-3100 ~ Total Organic Carbon in Water” in ResultAnalyticalMethod.MethodName Elemental Analyzer: Detection of “440” in ResultAnalyticalMethod.MethodName Carbonaceous Analyzer: Detection of “9060 A” in ResultAnalyticalMethod.MethodName Finally, we group the data into three tiers as described in Tiering, flagging, and quality control philosophy. These tiers are: Tier Name Description DOC details 0 Restrictive Data that are verifiably self-similar across organizations and time-periods and can be considered highly reliable and interoperable Includes records using Wet Oxidation+Persulfate+IR (EPA 415.3, SM 5310D) or Persulfate-UV/Heated Persulfate Oxidation+IR (EPA 415.2, SM 5310C) methods 1 Narrowed Data that we have good reason to believe are self-similar, but for which we can’t verify full interoperability across data providers Includes records using combustion+IR (EPA 415.1, SM 5310B) or elemental analyzer methods (EPA 440) 2 Inclusive Data that are assumed to be reliable and are harmonized to our best ability given the information available from the data provider. This tier includes NA or non-resolvable descriptions for the analytical method, which often make up the majority of methods descriptions for any given parameter Carbonaceous (EPA 9060A) and all other records by default, including NA methods At this point we export a file (3_harmonize/out/doc_tiering_record.csv) that contains a record of how specific method text was tiered and how many row counts corresponded to each method. 6.2.10 Flag based on field methods DOC doesn’t have field sampling methods that lend themselves well to comparing with analytical methods and assigning flags, unlike variables like chlorophyll a. We fill the field_flag column with NA for “not applicable” for DOC. No records should be removed by this process and so there are 0 rows dropped leaving 2.2 million remaining in the harmonized DOC dataset. 6.2.11 Miscellaneous flag Next we add a placeholder for the miscellaneous flag column, misc_flag. Similarly to the field_flag column, this will be filled with NA. Some parameters will have additional flagging requirements that DOC does not, so we include this placeholder to maintain the same columns across all parameter data products. No records should be removed by this process and so there are 0 rows dropped leaving 2.2 million remaining in the harmonized DOC dataset. 6.2.12 Remove unrealistic values Before finalizing the dataset we remove DOC values that are beyond a realistic threshold. We use 200 mg/L as our cutoff for removal \\[citation needed\\]. 19.2 thousand rows are removed. The final row count after this is 2.2 million. 6.2.13 Aggregate simultaneous records The final step of DOC harmonization is to aggregate simultaneous observations. Any group of samples determined to be simultaneous are simplified into a single record containing the mean and coefficient of variation (CV) of the group. These can be either true duplicate entries in the WQP or records with non-identical values recorded at the same time and place and by the same organization (field and/or lab replicates/duplicates). The CV can be used to filter the dataset based on the amount of variability that is tolerable to specific use cases. Note, however, that many entries will have a CV that is NA because there are no simultaneous records or 0 because the records are duplicates and all entries have the same harmonized_value. We identify simultaneous records to aggregate by creating identical subgroups (subgroup_id) from the following columns: parameter, OrganizationIdentifier, MonitoringLocationIdentifier, MonitoringLocationTypeName, ResolvedMonitoringLocationTypeName, ActivityStartDate, ActivityStartDateTime, ActivityStartTime.TimeZoneCode, harmonized_tz, harmonized_utc, harmonized_top_depth_value, harmonized_top_depth_unit, harmonized_bottom_depth_value, harmonized_bottom_depth_unit, harmonized_discrete_depth_value, harmonized_discrete_depth_unit, depth_flag, mdl_flag, approx_flag, greater_flag, tier, field_flag, misc_flag, harmonized_units. This selection limits the columns included in the final dataset, but we also provide a copy of the AquaMatch dataset prior to its aggregation (pipeline target p3_doc_preagg_grouped), and include the subgroup_id column, so that users can use the disaggregated data as well and match make joins between dataset versions. The final, aggregated values are presented in the harmonized_value and harmonized_value_cv columns. The number of rows used per group is recorded in the harmonized_row_count column. 0.3 million rows dropped leaving 1.8 million remaining in the final harmonized and aggregated doc dataset. 6.2.14 Harmonized DOC At this point the harmonization of the DOC data from the WQP is complete and we export the final dataset for use later in the workflow. Below is a final set of histograms showing the distribution of harmonized measurements broken down by tier after aggregating simultaneous records. Lastly, a series of maps showing the geographic distribution of records by tier: 6.2.14.1 Conterminous US 6.2.14.2 Alaska 6.2.14.3 Hawaii 6.2.14.4 American Samoa 6.2.14.5 Guam and the Commonwealth of the Northern Mariana Islands 6.2.14.6 Puerto Rico and the United States Virgin Islands "],["secchi-harmonization-process.html", "7 Secchi harmonization process 7.1 Pre-harmonization of the raw Secchi WQP dataset 7.2 Harmonization-ready Secchi dataset", " 7 Secchi harmonization process Following the completion of the {dataRetrieval} download process described previously, the pipeline contains raw WQP data for each parameter of interest. Before we harmonize each parameter we run through a series of universal “pre-harmonization” steps, which ensure that the datasets are appropriately formatted when entering their harmonization routines. The text below first walks through the pre-harmonization steps for the raw Secchi disk depth dataset and then delves into the specifics of the harmonization process. 7.1 Pre-harmonization of the raw Secchi WQP dataset At the start of the pre-harmonization process the raw Secchi disk depth WQP dataset contains 3.1 million rows. 7.1.1 Missing results Next, records that have missing data are dropped from the dataset. The row is flagged as missing if the result comment column contains any user-provided text indicating a missing value, currently including: analysis lost, not analyzed, not recorded, not collected, or no measurement taken. 0.7 thousand rows are dropped, resulting in a final count of 3.1 million. 7.1.2 Filter Status The final step in pre-harmonization is to filter the ResultStatusIdentifier column to include only the following statuses: \"Accepted\" \"Final\" \"Historical\" \"Validated\" \"Preliminary\" NA These statuses generally indicate a reliable result having been reached, however we also include NA in an effort to be conservative. More specifically, when making decisions for this and other columns we occasionally retain NA values if removing the records would otherwise drop 10% or more of the available data. This step removes 9.2 thousand rows of data, leaving it with 3.1 million rows remaining. 7.2 Harmonization-ready Secchi dataset Once ready for harmonization, the Secchi-only dataset contains the following user-defined characteristicNames: Depth, Secchi disk depth, Depth, Secchi disk depth (choice list), Secchi Reading Condition (choice list), Secchi depth, Water transparency, Secchi disc. These names are chosen in order to select for only those measurements that pertain to Secchi disk depth measurements. 7.2.1 Filter for water media We next ensure that the media type for all SDD data is \"Surface Water\", \"Water\", \"Estuary\", or NA. 2075 rows are removed. The final row count after this is 3.1 million. 7.2.2 Document and remove fails In this step we filter out records based on indications that they have failed data quality assurance or quality control for some reason given by the data provider (these instances are referred to here as “failures”). After reviewing the contents of the ResultCommentText, ResultMeasureValue_original (an unedited duplicate of the ResultMeasureValue field), and ResultDetectionConditionText columns, we developed a list of terms that captured the majority of instances where records had failures or unacceptable measurements. We found the phrasing to be consistent across columns and so we searched for the same (case agnostic) terms in all four locations. The terms are: “error”, “fail”, “invalid”, “no result”, “questionable”, “suspect”, “unable”, “reject”, “no data”, “Not Reported”, “no reading”, “-99”, “upper quantitation limit”. Below are pie charts that break down the number of failure detections by column. Note that the plotting below is automated so if one or more of the columns listed above are not plotted, this indicates that the column(s) did not return any matches for the failure phrases. Also note that a single record can contain multiple failure phrases; therefore, failure phrases are not mutually exclusive. 7.2.2.1 ResultCommentText fail detects 7.2.2.2 ResultDetectionConditionText fail detects 7.2.2.3 ResultMeasureValue_original fail detects 9.8 thousand rows are removed after detecting failure-related phrases and 3.1 million rows remain. 7.2.3 Clean special characters We next address the issue of special characters in the Secchi disk depth measurements. Some entries in the dataset contain special characters at the beginning of the measurement value, which need to be removed for proper numerical analysis. We do this by first identifying measurements with the special characters -, *, or =. Then we remove those special characters and convert the values to numeric values and note which records were modified. This should not result in a change in rows but we still check: 0 rows are removed and 3.1 million rows remain. 7.2.4 Clean values with “less than” data In this step, we find and clean records containing &lt; symbols. We first identify records where the numeric value is missing (NA) in the ResultMeasureValue column. We then look for the presence of a &lt; symbol followed by a number in the ResultMeasureValue_original column. If these conditions are met and there are no alphabetical characters in the original value, we extract the numeric value after the &lt; symbol. This extracted value is then used as the harmonized value for the measurement. This step is one component of the rules defining our mdl_flag column, so it is labeled here as “Clean MDLs” (“MDL” = Method detection limit). This should not result in a change in rows but we still check: 0 rows are removed and 3.1 million rows remain. 7.2.5 Clean values with “greater than” data We next proceed to clean up values reported as “greater than” a certain measurement. This is similar to the proceeding step of cleaning up “less than” data. The ResultMeasureValue column was checked for all three of the following conditions: Numeric-only version of the column was still NA after “&lt;” removal The original column text contained a number The original column text contained a “&gt;” The original column text did not have any letters in it We then used the numeric value (without “&gt;”) as the record’s harmonized_value moving forward. This should not result in a change in rows but we still check: 0 rows are removed and 3.1 million rows remain. 7.2.6 Gap filling harmonized values After cleaning up the “greater than” values, we proceed to fill missing values in the harmonized_value column using Secchi disk depth information present in ActivityBottomDepthHeightMeasure.MeasureValue, ActivityDepthHeightMeasure.MeasureValue, and their corresponding units columns. In order to gap-fill the harmonized_value column from the ActivityBottomDepthHeightMeasure we required both “bottom text” and no “negate text” to be present and for ActivityDepthHeightMeasure we required no “bottom text” present but instances of “negate text” was okay. “Bottom text” was defined as when the ActivityCommentText, ResultCommentText, and ResultMeasureValue columns contained language that referenced the “bottom” of the water body. Mentions of “bottom” alone were taken as a positive indicator that sampling hit the bottom, while the following other strings were used to negate such indication (“negate text”): “no bottom”, “not.bottom”, ”couldn’t.bottom”, “could not.bottom”, ”unable to.bottom”, “too deep”, “too shallow”, “doesn’t reach”, “did not reach”, “blocked”, “hidden”, “disappeared”, “hose frozen”, “pump frozen”, “cable not long enough”, “cord not long enough”, “won’t stay on bottom”, “current too strong”, “too windy”, “fast current”, “flooding”, “readings not on bottom”, “measurements not collected”, “sample taken at”, “depth too great”, “can’t get true bottom”, “seen on bottom: no”, “bottom reading not taken”, “afraid to touch bottom”, “forgot bottom reading”, “didn’t hit bottom”, “did not hit bottom”, “vegetation blocks”, “turbid”, “probe not on bottom”, “sonde.not.bottom”, “hydrolab not on bottom”, “profile not taken to bottom”, “unable to collect bottom”, “bottom.not.collected”, “bottom.not.recorded”. The strings listed here include the regex symbols of “.” for a single-character wildcard and “.*” for a greedy wildcard of any character length. After this, we flagged these records with the approx_flag, indicating the source of the gap filled values: 0: Value and units not adjusted (original harmonized_value is not NA) 1: harmonized_value filled using ActivityDepthHeightMeasure column 2: harmonized_value filled using ActivityBottomDepthHeightMeasure column This should not result in a change in rows but we still check: 0 thousand rows are removed and 3.1 million rows remain. 7.2.7 Drop unresolved NA measurements The goal of the preceding three steps was to prevent records with seemingly missing measurement data from being dropped if there was still a chance of recovering a usable value. At this point we’ve finished with that process and we proceed to check for remaining records with NA values in their harmonized_value column. If they exist, they are dropped. 51.7 thousand rows are removed. The final row count after this is 3 million. 7.2.8 Harmonize record units The next step in SDD harmonization is converting the units of WQP records. We create the following conversion table, which is used to translate units provided in WQP into meters (m): harmonized_units conversion m 1.0000 ft 0.3048 cm 0.0100 in 0.0254 mm 0.0010 feet 0.3048 meters 1.0000 mi 1609.3400 Below is a pie chart that breaks down the different unit codes that were dropped in the unit harmonization process, and how many records were lost with each code. Additionally we provide a set of histograms of harmonized measurements by CharacteristicName: 7.2.9 Clean and flag bottom depth data The next harmonization step involved gap filling the ActivityBottomDepthHeightMeasure.MeasureValue and ActivityBottomDepthHeightMeasure.MeasureUnitCode columns. There are four columns that explicitly contain depth information for a given WQP entry. Of these columns we are only interested in the ActivityBottomDepthHeightMeasure.MeasureValue column for Secchi disk depth because it may be an indicator of the bottom of the water body for a given WQP entry. Here we harmonize the values in the bottom depth column. To keep this data in line with the rest of the data in AquaSat v2, the other harmonized depth columns are filled with NA since Secchi disk depth is a discrete depth measurement. Those harmonized depth columns are: harmonized_top_depth_value harmonized_top_depth_unit harmonized_discrete_depth_value harmonized_discrete_depth_unit In order to gap fill bottom depths we searched for records with previously identified bottom-related language and no information in the bottom depth value and unit columns. These entries were flagged using the depth_flag column, where 0 = bottom depth value not adjusted and no indication that Secchi disk hit bottom, and 1 = bottom depth value back filled with harmonized_value due to indication that Secchi disk hit bottom. If an entry was flagged, then the harmonized_bottom_depth_value column was filled using the harmonized_value and the harmonized_bottom_depth_unit column was filled using the harmonized_units column. Finally, the units for the harmonized bottom depth values were converted to meters (m). This should not result in a change in rows but we still check: 0 thousand rows are removed and 3 million rows remain. 7.2.10 Filter methods After cleaning and standardizing the Secchi bottom depth measurements, we proceed to filter the analytical methods used. We check the ResultAnalyticalMethod.MethodName column for names that indicate non-Secchi disk depth measurements. Phrases used to flag and remove unrelated methods for Secchi disk depth data are: “analysis lost”, “not analyzed”, “not recorded”, “not collected”, “no measurement taken”. This process drops 11.3 thousand rows leaving 3 million remaining. 7.2.11 Tier methods We then tiered the data using information on the time of day of the measurement, whether a viewscope was used, and whether the harmonized_value was gap filled. Our tiers were informed by Davies-Colley et al. (1993). Specifically, 10am-2pm represents the ideal time range for measurements due to consistent lighting conditions and use of a viewscope can significantly improve measurement accuracy by reducing surface glare and wave effects. These considerations help assess the quality and reliability of the Secchi disk depth data across different measurement conditions. We tiered the data as follows: Tier Name Description SDD details 0 Restrictive Data that are verifiably self-similar across organizations and time-periods and can be considered highly reliable and interoperable Time was reported between 10am-2pm and was not 11:59:59. Method indicates use of viewscope 1 Narrowed Data that we have good reason to believe are self-similar, but for which we can’t verify full interoperability across data providers Either time reported was between 10am-2pm and was no 11:59:59, OR method indicates use of viewscope, but not both 2 Inclusive Data that are assumed to be reliable and are harmonized to our best ability given the information available from the data provider. Time not reported/was outside of 10am-2pm/was 11:59:59 and no indication that viewscope was used 3 Inclusive Same as above Data where the harmonized_value is gap filled from other columns in the record This should not result in a change in rows but we still check: 0 rows are removed and 3 million rows remain. 7.2.12 Flag After cleaning and tiering the Secchi data, we conduct a final flagging sweep. This process creates several flags to provide additional context and quality indicators for each measurement. We create four types of flags: mdl_flag: This flag indicates whether the value was corrected 1) because original WQP record contained a “&lt;” character or, 2) because the original value was less than 0.01m. flag 0: Value greater than 0.01m with NO &lt; character in original entry flag 1: Value greater than 0.01m with &lt; character in original entry flag 2: Value less than or equal to 0.01m with NO &lt; character in original entry; final value adjusted to to 0.01m flag 3: Value less than or equal to 0.01m with &lt; character in original entry; final value adjusted to 0.01m greater_flag: This flag indicates 1) whether the value was corrected because the original WQP record contained a “&gt;” character and 2), whether the harmonized_value is greater than 44m (Larson, 1972). Depths greater than 44m are flagged as potentially anomalous. flag 0: Value less than 44m with NO &gt; character flag 1: Value less than 44m with &gt; character flag 2: Value greater than 44m with NO &gt; character flag 3: Value greater than 44m with &gt; character field_flag: Indicates whether the ActivityCommentText or ResultCommentText columns contained language indicating environmental conditions, i.e.: “wind”, “chop”, “choppy”, “precipitation”, “rain”, “calm”, or “clear”. flag 0: Environmental indicator present in comment flag 1: No environmental indicator present in comments misc_flag: The miscellaneous flag captures various other conditions that might affect data quality or interpretation. flag 0: No miscellaneous condition flagged flag 1: Bottom of water body indicated flag 2: Harmonized value is greater than the bottom depth value flag 3: Special characters removed (-,=.*) This should not result in a change in rows but we still check: 0 rows are removed and 3 million rows remain. 7.2.13 Remove unrealistic values Before finalizing the dataset we removed Secchi disk depth values that were beyond a realistic threshold because they were likely erroneous. We used 80m as our cutoff for removal based on Gieskes et al. (1987). Additionally, we convert values less than 0.01m to 0.01m based on observed Secchi disk depth records (NALMS, 2024). These changes improve the overall reliability of the dataset by minimizing the amount of data with potential inaccuracies and preserving measurements that correctly describe the aquatic environment. This process drops 1819 rows leaving 3 million remaining. 7.2.14 Aggregate simultaneous records The final step of Secchi disk depth harmonization is to aggregate simultaneous observations. Any group of samples determined to be simultaneous are simplified into a single record containing the mean and coefficient of variation (CV) of the group. These can be either true duplicate entries in the Water Quality Portal (WQP) or records with non-identical values recorded at the same time and place and by the same organization (field and/or lab replicates/duplicates). The CV can be used to filter the dataset based on the amount of variability that is tolerable to specific use cases. Note, however, that many entries will have a CV that is NA because there are no duplicates or 0 because the records are duplicates and all entries have the same harmonized_value. We identify simultaneous records to aggregate by creating identical subgroups (subgroup_id) from the following columns: parameter, OrganizationIdentifier, MonitoringLocationIdentifier, MonitoringLocationTypeName, ResolvedMonitoringLocationTypeName, ActivityStartDate, ActivityStartTime.Time, ActivityStartTime.TimeZoneCode, harmonized_tz, harmonized_local_time, harmonized_utc, ActivityStartDateTime, harmonized_top_depth_value, harmonized_top_depth_unit, harmonized_bottom_depth_value, harmonized_bottom_depth_unit, harmonized_discrete_depth_value, harmonized_discrete_depth_unit, depth_flag, mdl_flag, approx_flag, greater_flag, tier, field_flag, misc_flag, harmonized_units. This selection limits the columns included in the final dataset, but we also provide a copy of the dataset prior to its aggregation (pipeline target p3_sdd_preagg_grouped), and including the subgroup_id column, so that users can use the non-aggregated data as well and make joins between dataset versions. The final, aggregated values are presented in the harmonized_value and harmonized_value_cv columns. The number of rows used per group is recorded in the harmonized_row_count column. 0.3 million rows dropped leaving 2.7 million remaining in the final harmonized and aggregated Secchi disk depth dataset. 7.2.15 Harmonized Secchi disk depth At this point the harmonization of the Secchi disk depth data from the WQP is complete and we export the final dataset for use later in the workflow. Below is a final set of histograms showing the distribution of harmonized measurements (top) and CVs (bottom) broken down by tier after aggregating simultaneous records. Lastly, a series of maps showing the geographic distribution of records by tier: 7.2.15.1 Conterminous US 7.2.15.2 Alaska 7.2.15.3 Hawaii 7.2.15.4 American Samoa 7.2.15.5 Guam and the Commonwealth of the Northern Mariana Islands 7.2.15.6 Puerto Rico and the United States Virgin Islands "],["notes.html", "8 Notes", " 8 Notes Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government. "],["references.html", "9 References", " 9 References Davies-Colley, R. J., W. N. Vant, and D. G. Smith. 1993. Colour and Clarity of Natural Waters: Science and Management of Optical Water Quality, Ellis Horwood. DeCicco, L. and others. 2024. dataRetrieval: R packages for discovering and retrieving water data available from U.S. federal hydrologic web services. doi:10.5066/P9X4L3GE Gieskes, W. W. C., C. Veth, A. Woehrmann, and M. Graefe. 1987. Secchi disc visibility world record shattered. Eos, Transactions American Geophysical Union 68: 123–123. doi:10.1029/EO068i009p00123-01 Landau, W. M. 2021. The targets R package: a dynamic Make-like function-oriented pipeline toolkit for reproducibility and high-performance computing. Journal of Open Source Software 6: 2959. Larson, D. W. 1972. Temperature, transparency, and phytoplankton productivity in Crater Lake, Oregon. Limnology and Oceanography 17: 410–417. doi:10.4319/lo.1972.17.3.0410 National Water Quality Monitoring Council, United States Geological Survey (USGS), and Environmental Protection Agency (EPA). 2021. Water Quality Portal.doi:https://doi.org/10.5066/P9QRKUVJ North American Lake Management Society (NALMS). Secchi Records. https://www.nalms.org/secchidipin/monitoring-methods/the-secchi-disk/secchi-records/. Accessed 2024-09-17. Ross, M. R. V., S. N. Topp, A. P. Appling, X. Yang, C. Kuhn, D. Butman, M. Simard, and T. M. Pavelsky. 2019. AquaSat: A Data Set to Enable Remote Sensing of Water Quality for Inland Waters. Water Resources Research 55: 10012–10025. doi:10.1029/2019WR024883 Teucher, A. 2023. lutz: Look Up Time Zones of Point Coordinates. R package version 0.3.2, https://CRAN.R-project.org/package=lutz. Wetzel, R. G. 2011. Limnology: lake and river ecosystems, 3. ed. San Diego: Academic Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
