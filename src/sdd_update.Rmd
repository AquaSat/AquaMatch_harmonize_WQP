---
title: "SDD"
author: "Daniel Dominguez"
format: 
  html:
    code-fold: true
    code-summary: "Show the code"
#page-layout: full
fig-width: 15
execute:
  echo: true
  warning: false
  error: false
  message: false
  cache: true
---

```{r, echo=FALSE}
tar_load(raw_sdd)
tar_load(p_codes)
```

## First step is to read in the data and make it workable, we'll then filter the data to 1984 and beyond

```{r}
raw_sdd <- raw_sdd %>% 
  select(
    date = ActivityStartDate,
    parameter = CharacteristicName,
    parm_cd = USGSPCode,
    units = ResultMeasure.MeasureUnitCode,
    SiteID = MonitoringLocationIdentifier,
    org = OrganizationFormalName,
    org_id = OrganizationIdentifier,
    time = ActivityStartTime.Time,
    value = ResultMeasureValue,
    sample_method = SampleCollectionMethod.MethodName,
    analytical_method = ResultAnalyticalMethod.MethodName,
    particle_size = ResultParticleSizeBasisText,
    date_time = ActivityStartDateTime,
    media = ActivityMediaName,
    type = ActivityMediaSubdivisionName,
    sample_depth = ActivityDepthHeightMeasure.MeasureValue,
    sample_depth_unit = ActivityDepthHeightMeasure.MeasureUnitCode,
    fraction = ResultSampleFractionText,
    status = ResultStatusIdentifier,
    field_comments = ActivityCommentText,
    lab_comments = ResultLaboratoryCommentText,
    result_comments = ResultCommentText
  ) %>%
  left_join(p_codes, by = "parm_cd") %>%
  mutate(year = year(date),
         units = trimws(units)) %>%
  filter(year >= 1984) %>%
  rowid_to_column(.,"index")

```

## Word Matrices

Developing word matrices to be used in identifying erroneous/failed data across relevant data columns. This is Michael Meyer's code. Essentially, we can use these matrices as justification for choosing our downstream `grepl` functions to pull out the erroneous/failed data.

```{r, eval=FALSE}

## Dont run this 
# Script for word frequency enumeration

gc()

vals <- str_replace_all(raw_SDD$field_comments, c("0|1|2|3|4|5|6|7|8|9" = ""))

# First create text_in with only the abstracts
text_in <- vals

# Remove NAs
text_in <- text_in[which(is.na(text_in)==FALSE)]

# For these procedures, object needs to be of class Corpus
text0 <- Corpus(x = VectorSource(x = text_in))

# Various text processing steps. 
text <-  TermDocumentMatrix(text0, 
                           control = 
                             list(removePunctuation = TRUE,
                                  stopwords = TRUE,
                                  tolower = TRUE,
                                  stemming = FALSE,
                                  removeNumbers = TRUE,
                                  bounds = list(global = c(1, Inf)))) 


# Find the frequency of certain terms that appear at least 
# 20 times, and then sort them in decreasing order. 
# The final outputted matrix will be saved as a CSV. 
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,]) 
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)

write.csv(sorted_matrix, "C:/Users/ddomi/Documents/Code/Aquasat/Aquasat_Processing/Data/Comments/SDD_field_comments_matrix.csv")

# LAB COMMENTS

# First create text_in with only the abstracts
text_in <- raw_SDD$lab_comments

# Remove NAs
text_in <- text_in[which(is.na(text_in)==FALSE)]

# For these procedures, object needs to be of class Corpus
text0 <- Corpus(x = VectorSource(x = text_in))

# Various text processing steps. 
text <-  TermDocumentMatrix(text0, 
                           control = 
                             list(removePunctuation = TRUE,
                                  stopwords = TRUE,
                                  tolower = TRUE,
                                  stemming = FALSE,
                                  removeNumbers = TRUE,
                                  bounds = list(global = c(1, Inf)))) 


# Find the frequency of certain terms that appear at least 
# 20 times, and then sort them in decreasing order. 
# The final outputted matrix will be saved as a CSV. 
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,]) 
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)

write.csv(sorted_matrix, "C:/Users/ddomi/Documents/Code/Aquasat/Aquasat_Processing/Data/Comments/SDD_lab_comments_matrix.csv")

# RESULT COMMENTS

# First create text_in with only the abstracts
text_in <- raw_SDD$result_comments

# Remove NAs
text_in <- text_in[which(is.na(text_in)==FALSE)]

# For these procedures, object needs to be of class Corpus
text0 <- Corpus(x = VectorSource(x = text_in))

# Various text processing steps. 
text <-  TermDocumentMatrix(text0, 
                           control = 
                             list(removePunctuation = TRUE,
                                  stopwords = TRUE,
                                  tolower = TRUE,
                                  stemming = FALSE,
                                  removeNumbers = TRUE,
                                  bounds = list(global = c(1, Inf)))) 


# Find the frequency of certain terms that appear at least 
# 20 times, and then sort them in decreasing order. 
# The final outputted matrix will be saved as a CSV. 
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,]) 
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)

write.csv(sorted_matrix, "C:/Users/ddomi/Documents/Code/Aquasat/Aquasat_Processing/Data/Comments/SDD_result_comments_matrix.csv")

# VALUE COMMENTS


vals <- str_replace_all(raw_SDD$value, c("0|1|2|3|4|5|6|7|8|9" = ""))

# First create text_in with only the abstracts
text_in <- vals

# Remove NAs
text_in <- text_in[which(is.na(text_in)==FALSE)]

# For these procedures, object needs to be of class Corpus
text0 <- Corpus(x = VectorSource(x = text_in))

# Various text processing steps. 
text <-  TermDocumentMatrix(text0, 
                           control = 
                             list(removePunctuation = TRUE,
                                  stopwords = TRUE,
                                  tolower = TRUE,
                                  stemming = FALSE,
                                  removeNumbers = TRUE,
                                  bounds = list(global = c(1, Inf)))) 


# Find the frequency of certain terms that appear at least 
# 20 times, and then sort them in decreasing order. 
# The final outputted matrix will be saved as a CSV. 
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,]) 
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)

write.csv(sorted_matrix, "C:/Users/ddomi/Documents/Code/Aquasat/Aquasat_Processing/Data/Comments/SDD_values_matrix.csv")
```

## Initial Data Cleaning Steps

First off, let's focus our data set to "water" samples (as done in original AquaSat). Then we can remove any samples from our data set that failed, or don't have enough lab metadata to make assumptions about the values presented. (In the future, these `grepl` functions can be based on the word matrices we just produced upstream!)

```{r}
trucolor_no_data_samples <- raw_sdd %>%
  # Identify samples that have no meaningful data related to an NA value
  filter(is.na(value) & is.na(units) & is.na(lab_comments) & is.na(result_comments))

trucolor_fails_removed <- raw_sdd %>%
  filter(media == "Water",
         status %in% c('Accepted', 'Final', 'Historical', 'Validated'),
         # No failure-related field comments, slightly different list of words
         # than lab and result list (not including things that could be used
         # to describe field conditions like "warm", "ice", etc.)
         !grepl(
           pattern = paste0(
             c("fail", "suspect", "error", "beyond accept", "interference",
               "questionable", "outside of accept", "problem", "contaminat",
               "improper", "violation", "invalid", "unable", "no test", "cancelled",
               "instrument down", "no result", "time exceed", "not accept",
               "QC EXCEEDED"),
             collapse = "|"),
           x = field_comments,
           ignore.case = T
         ) |
           is.na(field_comments),
         # No failure-related lab, should we remove controls comments
         !grepl(
           pattern = paste0(
             c("fail", "suspect", "error", "beyond accept", "interference",
               "questionable", "outside of accept", "problem", "contaminat",
               "improper", "warm", "violation", "invalid", "unable", "no test",
               "cancelled", "instrument down", "no result", "time exceed",
               "not accept", "QC EXCEEDED", "not ice", "ice melt",
               "PAST HOLDING TIME", "beyond", "exceeded", "failed", "exceededs"),
             collapse = "|"),
           x = lab_comments,
           ignore.case = T
         ) |
           is.na(lab_comments),
         # No failure-related result comments
         !grepl(
           pattern = paste0(
             c("fail", "suspect", "error", "beyond accept", "interference",
               "questionable", "outside of accept", "problem", "contaminat",
               "improper", "warm", "violation", "invalid", "unable", "no test",
               "cancelled", "instrument down", "no result", "time exceed",
               "not accept", "QC EXCEEDED", "not ice", "ice melt",
               "PAST HOLDING TIME", "null", "unavailable", "exceeded", "rejected"),
             collapse = "|"),
           x = result_comments,
           ignore.case = T
         ) | is.na(result_comments),
         # No failure-related values
         !grepl(
           pattern = paste0(
             c("fail", "suspect", "error", "beyond accept", "interference",
               "questionable", "outside of accept", "problem", "contaminat",
               "improper", "warm", "violation", "invalid", "unable", "no test",
               "cancelled", "instrument down", "no result", "time exceed",
               "not accept", "QC EXCEEDED", "not ice", "ice melt",
               "PAST HOLDING TIME"),
             collapse = "|"),
           x = value,
           ignore.case = T
         ) | is.na(value),
         # Remove samples that have no values and no lab/result metadata
         !index %in% trucolor_no_data_samples$index)
```

### The result of dropping the data based on various words in the lab, field, result comments

```{r}
print(
  paste(
    'We kept',
    round(nrow(trucolor_fails_removed) / nrow(raw_sdd) * 100, 2),
    '% of samples, because the method used did not make sense. These methods are:'
  )
)
```

```{r, echo=FALSE, message=FALSE}

rm(p_codes,trucolor_no_data_samples, raw_sdd) 
```

## Lastly we filter by non-sensical units only keeping those that make sense which are Platinum Cobalt Units and ADMI for the 2120E standard

Around this point I (MRB) start to be confused by whether this script is finished or a combination of multiple parameters, not just Secchi
